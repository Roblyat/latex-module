\chapter{Experimental and Results}
In \texttt{run\_full\_sweep.py}, training is organized as a Cartesian sweep over data regime, split ratio, and random seeds, followed by baseline physics learning (DeLaN) and residual learning (LSTM) with unified evaluation.

\begin{itemize}
    \item \textbf{Outer sweep (data regime).} For each trajectory budget $K \in \{25,50,75,100,150\}$, the script assigns epoch budgets $E_{\mathrm{DeLaN}}(K)$ and $E_{\mathrm{LSTM}}(K)$ that increase with $K$ (DeLaN: 150$\rightarrow$400 epochs; LSTM: 30$\rightarrow$100 epochs).
    
    \item \textbf{Split sweep.} For each test fraction $t_f \in \{0.2,0.3\}$ (with validation fraction fixed to $v_f=0.1$), an independent experimental branch is executed.
    
    \item \textbf{Dataset replication.} For each dataset seed $s \in \{0,1,2\}$, the raw CSV data is preprocessed into an NPZ dataset specific to $(K,t_f,v_f,s)$, defining train/validation/test partitions and derived signals.
    
    \item \textbf{DeLaN replication (seeded baseline learning).} For each DeLaN seed $s_d \in \{0,1,2\}$, a structured DeLaN (JAX implementation) is trained on the same NPZ split using the selected hyperparameter preset. The trained model is then used to export a residual dataset (torque residuals) for downstream learning.
    
    \item \textbf{Residual-model grid.} For every exported residual dataset, the script enumerates window lengths $H \in \{25,50\}$ and feature modes $\{\texttt{full},\texttt{tau\_hat},\texttt{state},\texttt{state\_tauhat}\}$. For each $(H,\mathrm{feat})$, it (i) builds supervised LSTM windows, (ii) trains an LSTM residual predictor, and (iii) evaluates the recombined model (DeLaN + residual correction) using a unified evaluation routine, writing per-run metrics and plots.
\end{itemize}


\textbf{The DeLaN seed determines the character assignment between train and test set, which is comparable to a k-fold evaluation.}
Since the dataset situation, this is tested for this dataset in current precision [dataport dataset with paper Juan H.]
And on Torque Prediction for X != 6 dof

\section{Model Visualizations}
\label{sec:model_visualizations}
This section compiles the recorded figures from the DeLaN, LSTM, and evaluation pipelines. The progression from training-specific diagnostics to end-to-end validation artifacts is used to interpret how baseline inverse-dynamics modeling and residual learning interact.

\textbf{DeLaN seed sensitivity and split effects.}
Across the experimental runs, the DeLaN seed influences the effective character assignment between train and test sets, which is comparable to a k-fold style evaluation: different seeds expose different trajectory segments and excitation patterns to training, thereby changing generalization difficulty. This effect motivates evaluating multiple DeLaN seeds and (in later pipeline iterations) selecting the best-performing DeLaN baseline before training the residual model.

\subsection{DeLaN Prediction Diagnostics}
\label{subsec:delan_diagnostics}

\paragraph{Torque predictions across training stages.}
The staged DeLaN training (structural stages s0, s1, s2) shows a clear refinement of the inverse-dynamics mapping. Early stages provide a coarse fit that captures dominant trends, while later stages improve phase alignment and amplitude tracking, reducing systematic mismatch across joints.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/delan/delan_UR3_Load0_combined_26__A__delan_jax_struct_s0_ep150__DeLaN_Torque.png}
\caption{Joint-wise torque traces predicted by DeLaN after structural stage s0 (epoch 150), showing the initial coarse fit of the inverse dynamics across all joints.}
\label{fig:delan_torque_s0}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/delan/delan_UR3_Load0_combined_26__A__delan_jax_struct_s1_ep250__DeLaN_Torque.png}
\caption{Torque prediction after structural refinement (stage s1, epoch 250). The model better captures oscillatory and transient regions, indicating improved identification of dynamic terms.}
\label{fig:delan_torque_s1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/delan/delan_UR3_Load0_combined_26__A__delan_jax_struct_s2_ep400__DeLaN_Torque.png}
\caption{Final DeLaN torque prediction (stage s2, epoch 400). The refined model tracks both steady and dynamic segments more consistently, reducing systematic mismatch across joints.}
\label{fig:delan_torque_s2}
\end{figure}

\paragraph{Kinematics and loss breakdown.}
Beyond torque traces, the train/test diagnostics highlight that generalization is not uniform across the trajectory distribution. Training loss can drop sharply while test error saturates or even worsens later, indicating sensitivity to split composition and potential overfitting. The loss-component plot further reveals how optimization balances data-fit and structural terms.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/delan/delan_UR3_Load0_combined_26__A__delan_jax_struct_s2_ep400__elbow_train_vs_test.png}
\caption{Training loss versus test-set MSE for an exemplary elbow joint at the final structural stage. The curve highlights rapid early improvement and later-stage generalization sensitivity.}
\label{fig:delan_elbow_train_test}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/delan/delan_UR3_Load0_combined_26__A__delan_jax_struct_s2_ep400__loss_components.png}
\caption{DeLaN training loss decomposition over epochs at the final structural stage. The plot visualizes how the optimization balances the data-fit term against structural regularizers.}
\label{fig:delan_loss_components}
\end{figure}

\paragraph{RMSE grids grouped by DeLaN tag.}
Aggregated RMSE grids grouped by DeLaN tag reveal clear separation between configurations: some identifiers generalize consistently across joints while others are systematically worse. This variance indicates that downstream residual learning is bounded by the baseline DeLaN quality and supports selection of a strong DeLaN prior to residual training.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/delan/box/delan_joint_rmse_grid_by_delan_tag.png}
\caption{Per-joint test RMSE grouped by DeLaN tag (identifier). The grid highlights seed/tag sensitivity and reveals configurations that generalize more consistently across joints.}
\label{fig:delan_rmse_grid}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/delan/box/delan_torque_rmse_by_delan_tag.png}
\caption{Test torque RMSE grouped by DeLaN tag. Differences across tags indicate variability in baseline inverse-dynamics quality prior to residual learning.}
\label{fig:delan_torque_rmse}
\end{figure}

\subsection{LSTM Feature Ablations}
\label{subsec:lstm_feature_ablations}

\paragraph{Feature-mode statistics.}
Residual learning performance depends on the feature representation used as input to the LSTM. The scatter of best validation loss versus total RMSE shows only a loose correlation, implying that scaled optimization loss is not a perfect proxy for physical-unit error. RMSE grids and box plots further show that richer feature sets are typically more robust, while reduced feature sets can increase variance and overfitting.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/lstm/box/lstm_best_val_loss_vs_rmse_total.png}
\caption{Scatter of best validation loss (scaled space) versus total residual RMSE (physical units) across LSTM runs. The loose correlation indicates that scaled loss is an imperfect proxy for real-unit performance.}
\label{fig:lstm_loss_vs_rmse}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/lstm/box/lstm_joint_rmse_grid_by_feature_mode.png}
\caption{Per-joint residual RMSE distributions for each feature mode. Richer feature sets achieve lower and more stable errors, while reduced feature sets exhibit larger variance and outliers.}
\label{fig:lstm_joint_rmse_grid}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/lstm/box/lstm_overfit_ratio_by_feature_mode.png}
\caption{Overfitting indicator (validation-to-training error ratio) grouped by feature mode. Higher ratios and heavy-tailed outliers suggest reduced generalization for certain feature configurations.}
\label{fig:lstm_overfit_ratio}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/lstm/box/lstm_residual_rmse_by_feature_mode.png}
\caption{Residual RMSE grouped by feature mode, summarizing how feature selection affects the standalone residual prediction quality on the test split.}
\label{fig:lstm_residual_rmse}
\end{figure}

\paragraph{Training diagnostics in the LSTM box folder.}
Representative training curves show validation loss saturation relative to training loss, consistent with limited benefits from additional epochs without model selection on best validation checkpoints. The residual time-series comparison indicates that the LSTM captures dominant residual trends while leaving systematic discrepancies in transient regions.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/lstm/loss_curve.png}
\caption{Representative LSTM training and validation loss curves. Validation loss saturation relative to training loss suggests limited gains from additional epochs without early stopping or stronger regularization.}
\label{fig:lstm_loss_curve}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/lstm/residual_gt_vs_pred.png}
\caption{Residual ground-truth versus LSTM prediction on the validation split. The LSTM captures dominant residual trends while leaving small systematic discrepancies in transient regions.}
\label{fig:lstm_residual_gt_pred}
\end{figure}

\subsection{Evaluation Residuals}
\label{subsec:evaluation_residuals}

\paragraph{Sensor residual analysis for trajectory windows.}
Comparing residual predictions across different window sizes indicates that larger temporal context improves residual reconstruction. The H=25 setting shows weaker tracking and larger deviations, whereas H=50 more faithfully matches phase and amplitude, supporting the use of larger H in subsequent sweeps.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/evaluation/residual_gt_vs_pred_test_H25.png}
\caption{Test-set residual comparison for window length H=25. Limited temporal context yields weaker tracking of residual structure and larger deviations from ground truth.}
\label{fig:eval_residual_H25}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/evaluation/residual_gt_vs_pred_test_H50.png}
\caption{Test-set residual comparison for window length H=50. Increased context improves phase and amplitude tracking, indicating more effective residual dynamics reconstruction.}
\label{fig:eval_residual_H50}
\end{figure}

\paragraph{Metric box plots from the evaluation box folder.}
The evaluation box plots summarize how baseline DeLaN quality and residual learning interact. DeLaN RMSE distributions show substantial variability across identifiers, implying that end-to-end performance is bounded by baseline quality. Gain and gain ratio plots show that residual learning typically yields positive improvements, with feature representation affecting both median gain and stability.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/evaluation/box/box_delan_rmse_by_delan_id.png}
\caption{Distribution of DeLaN baseline torque RMSE across DeLaN identifiers. The spread reflects sensitivity to training configuration and motivates selecting the best DeLaN prior to residual learning.}
\label{fig:eval_box_delan_rmse}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/evaluation/box/box_gain_by_feature_mode.png}
\caption{Absolute improvement gain ($\mathrm{gain}=\mathrm{RMSE}_{\mathrm{DeLaN}}-\mathrm{RMSE}_{\mathrm{RG}}$) grouped by feature mode. Positive medians indicate systematic benefit from residual learning, with occasional strong outliers.}
\label{fig:eval_box_gain}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/evaluation/box/box_gain_ratio_by_feature_mode.png}
\caption{Relative improvement expressed as gain ratio ($\mathrm{gain\_ratio}=\mathrm{RMSE}_{\mathrm{RG}}/\mathrm{RMSE}_{\mathrm{DeLaN}}$, lower is better). Feature mode influences both typical improvement and variability across runs.}
\label{fig:eval_box_gain_ratio}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/evaluation/box/box_res_rmse_by_feature_mode.png}
\caption{Residual-model RMSE ($\mathrm{RMSE}_{\mathrm{res}}$) grouped by feature mode. Lower values indicate more accurate residual estimation prior to recombination with the physics model.}
\label{fig:eval_box_res_rmse}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/evaluation/box/box_rg_rmse_by_feature_mode.png}
\caption{Final recombined torque RMSE ($\mathrm{RMSE}_{\mathrm{RG}}$) grouped by feature mode. This metric reflects end-to-end performance of DeLaN plus residual correction.}
\label{fig:eval_box_rg_rmse}
\end{figure}

\subsection{Evaluation Scatter Comparisons}
\label{subsec:evaluation_scatter}

\paragraph{Gain versus RMSE relationships.}
Scatter plots visualize how much improvement residual learning yields relative to baseline DeLaN quality. While weaker baselines sometimes offer more headroom for improvement, the relationship is not strictly monotonic and includes outliers. Strong correlations between baseline RMSE and final RMSE indicate that DeLaN quality sets the attainable floor, while residual RMSE correlates tightly with end-to-end error, indicating the residual model as a primary lever once a stable DeLaN baseline is established.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/evaluation/scatter/scatter_delan_rmse_vs_gain.png}
\caption{Scatter of DeLaN baseline RMSE versus absolute gain. The relationship assesses whether weaker physics baselines offer more headroom for residual correction.}
\label{fig:eval_scatter_rmse_vs_gain}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/evaluation/scatter/scatter_delan_rmse_vs_gain_ratio.png}
\caption{Scatter of DeLaN baseline RMSE versus gain ratio. Points below 1 indicate improvement; the distribution shows how relative improvement varies with baseline quality.}
\label{fig:eval_scatter_rmse_vs_gain_ratio}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/evaluation/scatter/scatter_delan_rmse_vs_gain_ratio__jax.png}
\caption{Gain ratio scatter restricted to JAX DeLaN runs, highlighting relative residual-correction effects under the JAX implementation.}
\label{fig:eval_scatter_rmse_vs_gain_ratio_jax}
\end{figure}

\paragraph{Residual gain comparisons.}
The final scatter comparisons show that (i) lower DeLaN RMSE strongly predicts lower end-to-end RMSE, and (ii) residual RMSE almost linearly determines the final recombined RMSE. Consequently, improving baseline physics learning stability and residual estimation quality are complementary levers for end-to-end torque prediction.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/evaluation/scatter/scatter_delan_rmse_vs_rg_rmse.png}
\caption{Scatter of DeLaN RMSE versus final recombined RMSE. The strong correlation indicates that baseline physics quality largely determines achievable end-to-end accuracy.}
\label{fig:eval_scatter_rmse_vs_rg_rmse}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{Images/05_results/evaluation/scatter/scatter_res_rmse_vs_rg_rmse.png}
\caption{Scatter of residual RMSE versus final recombined RMSE. The near-linear relationship shows that residual estimation accuracy translates directly into final torque prediction quality.}
\label{fig:eval_scatter_res_rmse_vs_rg_rmse}
\end{figure}

% \section{Model Visualizations}
% This section gathers the recorded figures for the DeLaN, LSTM, and evaluation pipelines so that the developments in model behavior can be followed from training-specific diagnostics to end-to-end validation artifacts.
% 
% \textbf{The DeLaN seed determines the character assignment between train and test set, which is comparable to a k-fold evaluation.}
% Since the dataset situation, this is tested for this dataset in current precision [dataport dataset with paper Juan H.]
% And on Torque Prediction for X != 6 dof
% 
% \subsection{DeLaN Prediction Diagnostics}
% \paragraph{Torque predictions across training stages.}
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/delan/delan_UR3_Load0_combined_26__A__delan_jax_struct_s0_ep150__DeLaN_Torque.png}
% \caption{DeLaN torque prediction after structural stage s0 at epoch 150.}
% \label{fig:delan_torque_s0}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/delan/delan_UR3_Load0_combined_26__A__delan_jax_struct_s1_ep250__DeLaN_Torque.png}
% \caption{Torque evolution following refinement at structural stage s1 (epoch 250).}
% \label{fig:delan_torque_s1}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/delan/delan_UR3_Load0_combined_26__A__delan_jax_struct_s2_ep400__DeLaN_Torque.png}
% \caption{Final torque trace for structural stage s2 after epoch 400.}
% \label{fig:delan_torque_s2}
% \end{figure}
% 
% \paragraph{Kinematics and loss breakdown.}
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/delan/delan_UR3_Load0_combined_26__A__delan_jax_struct_s2_ep400__elbow_train_vs_test.png}
% \caption{Comparison of elbow articulation during training and test data at the final structural stage.}
% \label{fig:delan_elbow_train_test}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/delan/delan_UR3_Load0_combined_26__A__delan_jax_struct_s2_ep400__loss_components.png}
% \caption{Architectural loss components tracked across the final training stage.}
% \label{fig:delan_loss_components}
% \end{figure}
% 
% \paragraph{RMSE grids grouped by DeLaN tag.}
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/delan/box/delan_joint_rmse_grid_by_delan_tag.png}
% \caption{Joint RMSE grid separated by DeLaN tag.}
% \label{fig:delan_rmse_grid}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/delan/box/delan_torque_rmse_by_delan_tag.png}
% \caption{Torque RMSE grid sorted by the DeLaN tag.}
% \label{fig:delan_torque_rmse}
% \end{figure}
% 
% \subsection{LSTM Feature Ablations}
% \paragraph{Feature-mode statistics.}
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/lstm/box/lstm_best_val_loss_vs_rmse_total.png}
% \caption{Best validation loss plotted against total RMSE for each feature mode.}
% \label{fig:lstm_loss_vs_rmse}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/lstm/box/lstm_joint_rmse_grid_by_feature_mode.png}
% \caption{Grid of joint RMSE values organized by feature extraction mode.}
% \label{fig:lstm_joint_rmse_grid}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/lstm/box/lstm_overfit_ratio_by_feature_mode.png}
% \caption{Overfit ratio measured per feature mode.}
% \label{fig:lstm_overfit_ratio}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/lstm/box/lstm_residual_rmse_by_feature_mode.png}
% \caption{Residual RMSE dissected by feature mode.}
% \label{fig:lstm_residual_rmse}
% \end{figure}
% 
% \paragraph{Training diagnostics in the LSTM box folder.}
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/lstm/loss_curve.png}
% \caption{Loss curve tracked during LSTM optimization.}
% \label{fig:lstm_loss_curve}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/lstm/residual_gt_vs_pred.png}
% \caption{Residuals of ground truth versus prediction curves on the validation split.}
% \label{fig:lstm_residual_gt_pred}
% \end{figure}
% 
% \subsection{Evaluation Residuals}
% \paragraph{Sensor residual analysis for trajectory H25 and H50.}
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/evaluation/residual_gt_vs_pred_test_H25.png}
% \caption{Residual comparison on the H25 trajectory test set.}
% \label{fig:eval_residual_H25}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/evaluation/residual_gt_vs_pred_test_H50.png}
% \caption{Residual comparison on the H50 trajectory test set.}
% \label{fig:eval_residual_H50}
% \end{figure}
% 
% \paragraph{Metric box plots from the evaluation box folder.}
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/evaluation/box/box_delan_rmse_by_delan_id.png}
% \caption{Delan RMSE distribution per identifier in the evaluation set.}
% \label{fig:eval_box_delan_rmse}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/evaluation/box/box_gain_by_feature_mode.png}
% \caption{Gain values stratified by feature mode.}
% \label{fig:eval_box_gain}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/evaluation/box/box_gain_ratio_by_feature_mode.png}
% \caption{Gain ratio comparison over the evaluated feature sets.}
% \label{fig:eval_box_gain_ratio}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/evaluation/box/box_res_rmse_by_feature_mode.png}
% \caption{Residual RMSE per feature mode documented as box plots.}
% \label{fig:eval_box_res_rmse}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/evaluation/box/box_rg_rmse_by_feature_mode.png}
% \caption{RG RMSE values grouped by feature mode.}
% \label{fig:eval_box_rg_rmse}
% \end{figure}
% 
% \subsection{Evaluation Scatter Comparisons}
% \paragraph{Gain versus RMSE curves.}
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/evaluation/scatter/scatter_delan_rmse_vs_gain.png}
% \caption{Scatter of Delan RMSE versus gain.}
% \label{fig:eval_scatter_rmse_vs_gain}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/evaluation/scatter/scatter_delan_rmse_vs_gain_ratio.png}
% \caption{Delan RMSE plotted against gain ratio.}
% \label{fig:eval_scatter_rmse_vs_gain_ratio}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/evaluation/scatter/scatter_delan_rmse_vs_gain_ratio__jax.png}
% \caption{Gain ratio comparison for the JAX implementation.}
% \label{fig:eval_scatter_rmse_vs_gain_ratio_jax}
% \end{figure}
% 
% \paragraph{Residual gain comparisons.}
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/evaluation/scatter/scatter_delan_rmse_vs_rg_rmse.png}
% \caption{Delan RMSE versus RG RMSE scatter.}
% \label{fig:eval_scatter_rmse_vs_rg_rmse}
% \end{figure}
% 
% \begin{figure}[H]
% \centering
% \includegraphics[width=\textwidth]{Images/05_results/evaluation/scatter/scatter_res_rmse_vs_rg_rmse.png}
% \caption{Residual RMSE against RG RMSE to emphasize the estimation gap.}
% \label{fig:eval_scatter_res_rmse_vs_rg_rmse}
% \end{figure}