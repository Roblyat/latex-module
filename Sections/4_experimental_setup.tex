\chapter{Experimental Setup}

\section{Dataset of Collaborative Robots}

\textbf{Dataset source and scope.}
All experiments are based on the publicly available ``Dataset of Collaborative Robots for
Energy Consumption Modeling'' released via IEEE DataPort~\cite{Dataset_Dataport} and
documented in~\cite{Q_Dataset_Paper}.
The dataset contains measurements from two Universal Robots platforms (UR3e and UR10e)
recorded both without load and with an external payload.

\textbf{Recorded signals.}
Each log sample provides a time stamp $t$ and a trajectory identifier, and includes joint-space
signals (joint positions $\mathbf{q}$, joint velocities $\dot{\mathbf{q}}$) together with electrical
measurements (per-joint motor currents $\mathbf{i}$, motor voltages, motor torques, as well as
robot-level current and voltage).
In addition, the dataset provides end-effector quantities such as Cartesian position and the
measured wrench (force and moment) at the end effector.
In the remainder of this thesis, motor current is treated as the central measured actuation signal.

\textbf{Data collection protocol.}
To excite the robot dynamics across a wide range of operating conditions, the robots execute
sinusoidal joint motions with varying amplitudes, frequencies, and initial conditions~\cite{Q_Dataset_Paper}:
\begin{equation}
  q_i(t) = q_{i0} + A_i \cos\!\bigl(2\pi f_i t + \varphi_i\bigr),
\end{equation}
where $q_i$, $q_{i0}$, $A_i$, $f_i$ and $\varphi_i$ denote the desired joint position, initial
position, amplitude, oscillation frequency, and phase of joint $i$, respectively.
The experiments were conducted for UR3e and UR10e under two load conditions: without load
and with an attached payload (hammer $1.5\,\mathrm{kg}$ and RobotiQ 2F-85 gripper $1\,\mathrm{kg}$)~\cite{Q_Dataset_Paper}.

\textbf{Sampling and dataset size.}
The signals are recorded at $100\,\mathrm{Hz}$.
For each robot (and load condition), the dataset provides $50{,}000$ samples for training and
$5{,}000$ samples for testing~\cite{Q_Dataset_Paper}.
Since the underlying dynamics are time-invariant, the published dataset is formed by combining
multiple shorter recordings into one consistent dataset, without treating discontinuities as
separate experiments~\cite{Q_Dataset_Paper}.

\section{DeLaN + LSTM - Learning Curve Stroy}

\textbf{Key findings.}
The learning-curve study shows that trajectory quantity $K$ is a primary driver of stability and generalisation for both stages: small subsets lead to higher dispersion and occasional failure modes, whereas sufficiently large $K$ yields consistent convergence and low per-joint errors.
In this regime, freezing the best DeLaN and learning residual dynamics with an LSTM provides an additional systematic reduction of the remaining modelling error.

\subsection{K-Domination}

\textbf{Motivation and experimental protocol.}
The purpose of the K-domination study is to quantify how the number of available
demonstration trajectories influences the complete two-stage pipeline
(Stage~1 DeLaN and Stage~2 residual LSTM).
To this end, we construct multiple training sets by drawing $K$ trajectories
from a fixed pool, train the full pipeline for each set under identical
hyperparameters, and evaluate performance as a function of $K$.

\textbf{Trajectory pool and signals.}
The base dataset consists of $122$ trajectories, each identified by a trajectory
ID.
For each trajectory, we use joint position $\mathbf{q}$, joint velocity
$\dot{\mathbf{q}}$, and motor current $\mathbf{i}$.
All logs are recorded at approximately $100\,\mathrm{Hz}$.
Since trajectory durations vary substantially (roughly $5$--$40\,\mathrm{s}$),
all filtering and preprocessing steps are applied per trajectory.

\textbf{Per-trajectory low-pass filtering.}
To attenuate sensor noise while avoiding temporal misalignment, we apply a
4th-order Butterworth low-pass filter with cutoff frequency
$f_c = 10\,\mathrm{Hz}$.
The filter is applied in zero-phase form (forward--backward filtering), thereby
preventing phase shifts in $\mathbf{q}$, $\dot{\mathbf{q}}$ and $\mathbf{i}$.
Joint accelerations $\ddot{\mathbf{q}}$ are derived from the filtered velocities
via numerical differentiation and, if filtering is enabled, are filtered
analogously.

\textbf{Subsampling by $K$.}
From the trajectory pool, we draw subsets of size
\[
  K \in \{8,\,16,\,32,\,48,\,64,\,84,\,122\},
\]
using three independent dataset seeds.
For each $(K,\mathrm{seed})$, a fixed trajectory split is created with
test fraction $0.2$ and validation fraction $0.1$ at the level of complete
trajectories (i.e., trajectories are never split across subsets).

\textbf{Stage~1 (DeLaN) configuration and model selection.}
For each $(K,\mathrm{seed})$ subset, we train five DeLaN initialisations
(seeds $s\in\{0,\dots,4\}$) and select the best DeLaN by validation error
(validation torque RMSE, equivalently MSE).
All DeLaN runs use the structured JAX implementation and a fixed hyperparameter
preset \texttt{lutter\_like\_256} (Table 1~\cite{Q4_2_lutter2023combiningphysicsdeeplearning}):
softplus activation, width $256$, depth $2$, mini-batch size $1024$,
learning rate $10^{-4}$ and weight decay $10^{-5}$.
Training is run for at most $200$ epochs with early stopping (patience $10$,
monitored on validation MSE) to avoid overfitting and to ensure that changes in
performance are attributable to $K$ rather than excessive training time.

\textbf{Stage~2 (LSTM) configuration.}
After selecting the best DeLaN, we freeze it and export residuals for each
trajectory.
Stage~2 uses a history length $H=100$ and feature mode \texttt{full}, i.e.,
each LSTM input time step concatenates
$(\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}},\hat{\boldsymbol{\tau}}_{\mathrm{DeLaN}})$.
The residual LSTM is trained with two stacked LSTM layers (units $128$), dropout
$0.2$, batch size $64$, and a validation split of $0.1$.
Training runs for at most $120$ epochs and employs early stopping on validation
loss (patience $20$, warm-up $5$ epochs), again fixing hyperparameters across
all $K$ to isolate the effect of trajectory quantity.

\begin{algorithm}[H]
\caption{K-domination experiment protocol for the DeLaN+LSTM pipeline}\label{alg:k_domination_protocol}
\begin{algorithmic}
\Require Trajectory pool $\mathcal{T}$ with $|\mathcal{T}|=122$ at $100\,\mathrm{Hz}$
\Require $K \in \{8,16,32,48,64,84,122\}$, dataset seeds $\mathcal{D}=\{0,1,2\}$
\Require DeLaN seeds $\mathcal{S}_{\mathrm{DeLaN}}=\{0,\dots,4\}$, LSTM seeds $\mathcal{S}_{\mathrm{LSTM}}=\{0,\dots,4\}$
\Ensure Aggregated learning curves and per-joint RMSE as a function of $K$

\State Low-pass filter each trajectory ($4^{\mathrm{th}}$-order Butterworth, $f_c=10\,\mathrm{Hz}$, zero-phase)
\ForAll{$K$}
    \ForAll{$d \in \mathcal{D}$}
        \State Sample $K$ trajectories from $\mathcal{T}$ using seed $d$
        \State Split into train/val/test trajectories (fixed fractions, no within-trajectory splitting)
        \ForAll{$s \in \mathcal{S}_{\mathrm{DeLaN}}$}
            \State Train DeLaN with fixed hyperparameters \Comment{max 200 epochs; early stop (patience 10)}
            \State Record train loss and validation MSE/RMSE
        \EndFor
        \State Select best DeLaN by validation error and freeze its parameters
        \State Export residuals per trajectory
        \ForAll{$s \in \mathcal{S}_{\mathrm{LSTM}}$}
            \State Build residual windows ($H=100$) and train LSTM \Comment{max 120 epochs; early stop (patience 20)}
            \State Record train/validation loss and residual RMSE
        \EndFor
    \EndFor
    \State Aggregate across seeds: median $\pm$ IQR learning curves and progress-aligned errors
\EndFor
\end{algorithmic}
\end{algorithm}

\textbf{Aggregation and reporting.}
For each $K$, we aggregate results across the three dataset seeds.
Reported learning curves (train loss, validation loss) are shown as median
curves with interquartile ranges (IQR, $25$--$75$ percentile) across the 5 delan seeds per $(K,\mathrm{seed})$.
For Stage~1, where five DeLaN initialisations are trained per $(K,\mathrm{seed})$,
we first compute the median $\pm$ IQR across DeLaN seeds for each dataset seed,
and then aggregate these seed-wise median curves across dataset seeds.
For time-dependent error visualisations, trajectories are aligned by normalised
progress (mapping each trajectory time index to $[0,1]$) and resampled to a
fixed number of bins; median $\pm$ IQR is then computed per progress bin.

\subsection{Best Model Approch}

\textbf{Motivation.}
The K-domination sweep shows that once a sufficiently large number of trajectories is available,
the DeLaN training dynamics become comparable across $K$ and the remaining variance is dominated
by (i) which trajectories end up in the train/validation/test split (dataset seed) and (ii) the
network initialisation (DeLaN seed).
This effect is visible in the collapse of the median learning curves for $K\geq 32$
(Figures~\ref{fig:kdom_delan_train_loss_by_k} and~\ref{fig:kdom_delan_val_mse_by_k}), while the
dispersion and occasional outliers remain seed-dependent, especially for the progress-aligned
torque RMSE (Figure~\ref{fig:kdom_delan_rmse_progress_by_k}).

\textbf{Objective.}
To obtain a reliable basis for the second experiment, we therefore fix $K=84$ and select a DeLaN
hyperparameter preset within the stable $K$ regime.
Since Algorithm~\ref{alg:best_model_approach} performs validation-based checkpoint selection, the
hyperparameters should (i) reduce seed sensitivity (tight IQR with few or no catastrophic runs),
(ii) reach low validation error quickly and consistently (stable early-stopping behaviour), and
(iii) generalise such that the validation-based ranking correlates with test performance.
Accordingly, we do not select the setting with the lowest median validation error alone, but the
setting that achieves a favourable accuracy--robustness trade-off across dataset and initialisation
seeds.

\textbf{Robust DeLaN score for hyperparameter selection.}
For each DeLaN preset $h$, we aggregate validation RMSE across DeLaN seeds (and then across dataset
seeds) and compute a robustness-aware selection score
\begin{equation}
  \mathrm{score}(h)
  =
  \mathrm{median}\!\left(\mathrm{RMSE}_{\mathrm{val}}\right)
  + \lambda \cdot \mathrm{IQR}\!\left(\mathrm{RMSE}_{\mathrm{val}}\right)
  + P \cdot \mathbf{1}\!\left\{\text{diverged run under } h\right\}.
\end{equation}
We use $\lambda=0.5$ and $P=10$ (in current units [A]).
Here, the median term captures typical validation accuracy, the IQR term penalises sensitivity to
dataset and initialisation seeds, and the penalty term excludes hyperparameter settings that exhibit
numerical divergence (e.g., NaNs or exploding loss) for any seed.
Lower values of $\mathrm{score}(h)$ therefore indicate both high accuracy and high reliability.

\textbf{Aggregate scatter plots.}
In addition to the scalar score, we report two aggregated scatter plots for comparing presets $h$:
(i) median validation RMSE versus IQR of validation RMSE, and (ii) median validation RMSE versus
median test RMSE.
In both cases, statistics are computed across DeLaN seeds for each dataset seed first, and then
aggregated across dataset seeds, such that each point reflects the combined accuracy--robustness
behaviour under the dominant seed variability identified by K-domination.
The bottom-left region of the median--IQR plot corresponds to accurate and stable settings, while
the validation--test plot directly assesses whether validation error is a reliable selector for
test performance in the sense required by Algorithm~\ref{alg:best_model_approach}.

\textbf{DeLaN presets ($|\mathcal{H}_{\mathrm{DeLaN}}|=5$).}
Guided by the stable ``Lutter-like'' regime observed in the K-domination results for $K\geq 32$
(Figures~\ref{fig:kdom_delan_train_loss_by_k}--\ref{fig:kdom_delan_rmse_per_joint}), we evaluate the
following five presets at fixed $K=84$:
\begin{enumerate}
    \item \textbf{Baseline (Lutter-like):} SoftPlus, batch size $1024$, learning rate $10^{-4}$, weight decay $10^{-5}$, width $256$, depth $2$.
    \item \textbf{Smaller capacity:} baseline with width $128$, depth $2$.
    \item \textbf{Deeper network:} baseline with width $256$, depth $3$.
    \item \textbf{More regularisation:} baseline with weight decay $10^{-4}$.
    \item \textbf{Lower step size:} baseline with learning rate $5\cdot 10^{-5}$.
\end{enumerate}
This best-model study thus fixes $K=84$ and asks: \emph{within the stable regime}, which
hyperparameters provide the best accuracy--robustness trade-off across dataset splits and DeLaN
initialisations?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\textbf{Stage 2: Should we already set up the LSTM best-model loop?}
Yes—\emph{you can set up Stage 2 now} without first having identified the single best DeLaN configuration, as long as Stage~2 is formulated as a \emph{conditional} optimization: \emph{given} a reasonably stable DeLaN baseline (selected by validation error), we then optimize the residual learner and quantify the added value.

This is consistent with the standard two-stage residual-compensation logic used in the literature: (i) learn a physics-based inverse dynamics model, (ii) learn a data-driven residual term capturing unmodeled effects such as friction and other nonlinearities. :contentReference[oaicite:0]{index=0}

\paragraph{Why Stage 2 is already meaningful (even if DeLaN is not fully “final”).}
\begin{itemize}
    \item Your K-domination experiments indicate that DeLaN variance is dominated by the data regime $K$ (and by fold/seed effects at low $K$). This means you already know the correct strategy for Stage~2: \textbf{freeze a strong DeLaN regime} (high $K$) and then study what the residual learner can add \emph{in a stable baseline setting}.
    \item In related robotic torque/force estimation work, LSTM-style models are trained with \textbf{early stopping based on validation loss} (e.g., stop when validation loss does not improve for 5 consecutive epochs) and sequence length was found to matter (longer histories improved performance). 
    \item Also, including torque-related signals as inputs can materially improve sequential models (reported improvement on the order of $\sim$15\% in one study), which directly motivates testing feature modes that \textbf{include $\hat{\tau}$} vs.\ those that do not. :contentReference[oaicite:2]{index=2}
\end{itemize}

\subsection{Recommended Stage 2 “best LSTM” experiment design}
The goal is to identify an LSTM configuration that (i) reliably converges (no premature stopping), (ii) yields consistent gains over the DeLaN baseline, and (iii) generalizes across dataset splits.

\paragraph{Core principle: nested selection.}
Do \textbf{not} sweep LSTM hyperparameters on weak DeLaN runs. Instead:
\begin{enumerate}
    \item For each dataset split (dataset seed), train multiple DeLaN initializations.
    \item Select the best DeLaN using validation torque RMSE (or MSE equivalently).
    \item Only then train the residual LSTM(s) on the residuals of the selected DeLaN.
\end{enumerate}
This reduces wasted LSTM training and makes LSTM comparisons interpretable.

\paragraph{Stage 2 sweep axes (what to vary).}
Given your observations (H=25 fails; patience too small stops at 5--15 epochs), the highest-value sweep is:

\begin{itemize}
    \item \textbf{History length $H$ (primary axis).} 
    Use a small set biased toward longer contexts, e.g.
    \[
    H \in \{50, 100, 150, 200\}.
    \]
    Rationale: longer sequences improved sequential model performance in related work. :contentReference[oaicite:3]{index=3}

    \item \textbf{Early stopping policy (primary axis).}
    Your current policy (warm-up $=0$, patience $=10$) can stop too early.
    Use a \emph{minimum-training constraint} plus patience:
    \[
    \text{warm-up} \in \{10, 20\}, \quad \text{patience} \in \{20, 30\}.
    \]
    This matches the general practice of validation-based stopping (commonly used in torque/force estimation networks) but avoids premature termination. :contentReference[oaicite:4]{index=4}

    \item \textbf{Feature modes (secondary axis, but important for interpretation).}
    For a thesis-ready story, I would run:
    \[
    \texttt{full},\ \texttt{state},\ \texttt{tau\_hat},\ \texttt{state\_tauhat}.
    \]
    Motivation: papers report that adding torque(-like) signals can help sequential estimators. :contentReference[oaicite:5]{index=5}
    In your case, $\hat{\tau}_{\mathrm{DeLaN}}$ is the torque-like proxy; this makes the comparison scientifically meaningful:
    \begin{itemize}
        \item \texttt{state} tests whether residuals are explainable purely from kinematics.
        \item \texttt{tau\_hat} tests whether the residual mainly depends on the baseline prediction.
        \item \texttt{full} tests whether kinematics + baseline together is best.
    \end{itemize}

    \item \textbf{Model capacity (secondary axis).}
    Keep most LSTM capacity fixed initially to isolate $H$ and early stopping:
    \[
    \text{units}=128,\ \text{dropout}=0.2,\ \text{layers}=2
    \]
    and only if results saturate, test one larger and one smaller setting, e.g.\ units $\in\{64,128,256\}$.

    \item \textbf{Seeds.}
    For Stage 2, add \textbf{LSTM seeds} only after narrowing to 1--2 best configs:
    \[
    s_{\mathrm{LSTM}} \in \{0,1,2\}
    \]
    is usually enough to quantify residual-model variance.
\end{itemize}

\subsection{How to define the Stage 2 selection metric (what “best” means)}
To match your pipeline and avoid leakage:

\begin{itemize}
    \item Select the \textbf{best LSTM config} by minimizing \textbf{validation residual RMSE} (or validation loss if your loss is residual MSE).
    \item Report final performance on the test set via your combined evaluation metrics:
    \[
    \mathrm{RMSE}(\tau_{\mathrm{gt}}, \tau_{\mathrm{DeLaN}}),\quad
    \mathrm{RMSE}(\tau_{\mathrm{gt}}, \tau_{\mathrm{DeLaN}}+\hat{r}),
    \]
    and derived gain metrics.
\end{itemize}

\subsection{Implementation notes that matter for correctness}
% \begin{itemize}
%     \item \textbf{Avoid Unicode combining characters in \LaTeX:} use $\ddot{q}$ instead of $q̈$.
%     \item \textbf{Filtering/derivatives:} Lutter explicitly computes velocities/accelerations via finite differences and low-pass filtering (offline, zero-phase) to avoid phase shift. 
%     This supports your approach of filtered signals before residual learning.
% \end{itemize}

\subsection{A concrete Stage 2 “best LSTM” loop (thesis-ready)}
With DeLaN fixed to a stable regime (e.g., your lutter-like preset and high $K$):

\begin{enumerate}
    \item Fix a stable DeLaN regime (high $K$) and select best DeLaN per dataset seed using validation RMSE.
    \item For each selected DeLaN:
    \begin{enumerate}
        \item Sweep $H \in \{50,100,150,200\}$.
        \item Sweep early stopping settings: warm-up $\in\{10,20\}$, patience $\in\{20,30\}$.
        \item Sweep feature modes: \texttt{full}, \texttt{state}, \texttt{tau\_hat}, \texttt{state\_tauhat}.
        \item Train LSTM, select best checkpoint on validation.
        \item Run combined evaluation and store $(\mathrm{delan\_rmse}, \mathrm{rg\_rmse}, \mathrm{gain}, \mathrm{gain\_ratio})$.
    \end{enumerate}
    \item Pick the final Stage~2 config as the one with the best (median) test gain ratio and low dispersion (IQR) across dataset seeds.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[H]
\caption{Best-model selection for the DeLaN+LSTM pipeline}\label{alg:best_model_approach}
\begin{algorithmic}
\Require Trajectory pool size $K=84$ with fixed split $(N_{\mathrm{train}},N_{\mathrm{val}},N_{\mathrm{test}})=(59,8,17)$
\Require Dataset seeds $\mathcal{D}=\{0,1,2,3,4\}$, DeLaN seeds $\mathcal{S}_{\mathrm{DeLaN}}=\{0,1,2,3,4\}$, LSTM seeds $\mathcal{S}_{\mathrm{LSTM}}=\{0,1,2,3,4\}$
\Require Hyperparameter presets $\mathcal{H}_{\mathrm{DeLaN}}$ and $\mathcal{H}_{\mathrm{LSTM}}$ with $|\mathcal{H}_{\cdot}|=5$
\Ensure Best DeLaN checkpoint and best LSTM checkpoint for the fixed $K$

	\State \textbf{Stage 1: DeLaN model selection}
	\ForAll{$d \in \mathcal{D}$}
	    \ForAll{$h \in \mathcal{H}_{\mathrm{DeLaN}}$}
	        \ForAll{$s \in \mathcal{S}_{\mathrm{DeLaN}}$}
	            \State Train DeLaN on split $(59,8,17)$ \Comment{log train/val curves; early stopping}
	            \State Record $\mathrm{RMSE}_{\mathrm{val}}$ and $\mathrm{RMSE}_{\mathrm{test}}$ \Comment{flag diverged runs}
	        \EndFor
	        \State Compute seed-wise statistics for $(d,h)$ and append to aggregation buffers:
	        \Statex \hspace{1.25em}$\tilde{r}_{\mathrm{val}}(d,h)=\mathrm{median}_{s}\!\left(\mathrm{RMSE}_{\mathrm{val}}\right)$,\;
	        $q_{\mathrm{val}}(d,h)=\mathrm{IQR}_{s}\!\left(\mathrm{RMSE}_{\mathrm{val}}\right)$,\;
	        $\tilde{r}_{\mathrm{test}}(d,h)=\mathrm{median}_{s}\!\left(\mathrm{RMSE}_{\mathrm{test}}\right)$
	        \State Save best DeLaN for this $(K,d)$ by validation error
	    \EndFor
	\EndFor
	\State Aggregate across dataset seeds for each $h$:
	\Statex \hspace{1.25em}$x_h=\mathrm{median}_{d}\!\left(\tilde{r}_{\mathrm{val}}(d,h)\right)$,\;
	$y^{\mathrm{IQR}}_h=\mathrm{median}_{d}\!\left(q_{\mathrm{val}}(d,h)\right)$,\;
	$y^{\mathrm{test}}_h=\mathrm{median}_{d}\!\left(\tilde{r}_{\mathrm{test}}(d,h)\right)$
	\State Plot \texttt{scatter\_valrmse\_median\_vs\_iqr\_\_delan} \Comment{$x$: $x_h$ (median val RMSE), $y$: $y^{\mathrm{IQR}}_h$ (IQR of val RMSE)}
	\State Plot \texttt{scatter\_valrmse\_median\_vs\_testrmse\_median\_\_delan} \Comment{$x$: $x_h$ (median val RMSE), $y$: $y^{\mathrm{test}}_h$ (median test RMSE)}
	\State Save best-model plots for DeLaN \Comment{exemplar overlays; see \texttt{delan\_plots.py}}

\State \textbf{Freeze DeLaN and export residuals}
\State Freeze best DeLaN parameters and export residual torques per trajectory

\State \textbf{Stage 2: LSTM model selection (same split and $K$)}
\ForAll{$d \in \mathcal{D}$}
    \ForAll{$h \in \mathcal{H}_{\mathrm{LSTM}}$}
        \ForAll{$s \in \mathcal{S}_{\mathrm{LSTM}}$}
            \State Build LSTM windows from exported residuals (same split)
            \State Train residual LSTM \Comment{log train/val curves; early stopping}
            \State Record validation metric and append to aggregation buffers
        \EndFor
        \State Update aggregate plots for this $(K,d,h)$ \Comment{median $\pm$ IQR; see \texttt{lstm\_plots.py}}
        \State Save best LSTM for this $(K,d)$ by validation loss
    \EndFor
\EndFor
\State Save best-model plots for LSTM \Comment{residual overlays; see \texttt{lstm\_plots.py}}

\State \textbf{Combined evaluation}
\State Evaluate DeLaN+LSTM on the test split and store final metrics/plots
\end{algorithmic}
\end{algorithm}
