\chapter{Experimental Setup}

\textbf{Measurement convention (motor current domain).}
From this chapter onwards, we formulate training objectives and evaluation metrics in the measured actuation domain provided by the dataset, i.e., per-joint motor currents $\mathbf{i}_{\mathrm{motor}}$ in $\mathrm{A}$.
When needed for interpretation, current-domain errors can be mapped to equivalent motor-domain errors via the (previously introduced) constant conversion factor $k_t$, i.e., by multiplying with $k_t$.

\section{Dataset of Collaborative Robots}

\textbf{Dataset source and scope.}
All experiments are based on the publicly available ``Dataset of Collaborative Robots for
Energy Consumption Modeling'' released via IEEE DataPort~\cite{Dataset_Dataport} and
documented in~\cite{Q_Dataset_Paper}.
The dataset contains measurements from two Universal Robots platforms (UR3e and UR10e)
recorded both without load and with an external payload.

\textbf{Recorded signals.}
Each log sample provides a time stamp $t$ and a trajectory identifier, and includes joint-space
signals (joint positions $\mathbf{q}$, joint velocities $\dot{\mathbf{q}}$) together with electrical
measurements (per-joint motor currents $\mathbf{i}$, motor voltages, as well as
robot-level current and voltage).
In addition, the dataset provides end-effector quantities such as Cartesian position and the
measured wrench (force and moment) at the end effector.
In the remainder of this thesis, motor current is treated as the central measured actuation signal.

\textbf{Data collection protocol.}
To excite the robot dynamics across a wide range of operating conditions, the robots execute
sinusoidal joint motions with varying amplitudes, frequencies, and initial conditions~\cite{Q_Dataset_Paper}:
\begin{equation}
  q_i(t) = q_{i0} + A_i \cos\!\bigl(2\pi f_i t + \varphi_i\bigr),
\end{equation}
where $q_i$, $q_{i0}$, $A_i$, $f_i$ and $\varphi_i$ denote the desired joint position, initial
position, amplitude, oscillation frequency, and phase of joint $i$, respectively.
The experiments were conducted for UR3e and UR10e under two load conditions: without load
and with an attached payload (hammer $1.5\,\mathrm{kg}$ and RobotiQ 2F-85 gripper $1\,\mathrm{kg}$)~\cite{Q_Dataset_Paper}.

\textbf{Sampling and dataset size.}
The signals are recorded at $100\,\mathrm{Hz}$.
For each robot (and load condition), the dataset provides $50{,}000$ samples for training and
$5{,}000$ samples for testing~\cite{Q_Dataset_Paper}.
Since the underlying dynamics are time-invariant, the published dataset is formed by combining
multiple shorter recordings into one consistent dataset, without treating discontinuities as
separate experiments~\cite{Q_Dataset_Paper}.

\section{DeLaN + LSTM - Learning Curve Stroy}

\textbf{Key findings.}
The learning-curve study shows that trajectory quantity $K$ is a primary driver of stability and generalisation for both stages: small subsets lead to higher dispersion and occasional failure modes, whereas sufficiently large $K$ yields consistent convergence and low per-joint errors.
In this regime, freezing the best DeLaN and learning residual dynamics with an LSTM provides an additional systematic reduction of the remaining modelling error.

\subsection{K-Domination}

\textbf{Motivation and experimental protocol.}
The purpose of the K-domination study is to quantify how the number of available
demonstration trajectories influences the complete two-stage pipeline
(Stage~1 DeLaN and Stage~2 residual LSTM).
To this end, we construct multiple training sets by drawing $K$ trajectories
from a fixed pool, train the full pipeline for each set under identical
hyperparameters, and evaluate performance as a function of $K$.

\textbf{Trajectory pool and signals.}
The base dataset consists of $122$ trajectories, each identified by a trajectory
ID.
For each trajectory, we use joint position $\mathbf{q}$, joint velocity
$\dot{\mathbf{q}}$, and motor current $\mathbf{i}$.
All logs are recorded at approximately $100\,\mathrm{Hz}$.
Since trajectory durations vary substantially (roughly $5$--$40\,\mathrm{s}$),
all filtering and preprocessing steps are applied per trajectory.

\textbf{Per-trajectory low-pass filtering.}
To attenuate sensor noise while avoiding temporal misalignment, we apply a
4th-order Butterworth low-pass filter with cutoff frequency
$f_c = 10\,\mathrm{Hz}$.
The filter is applied in zero-phase form (forward--backward filtering), thereby
preventing phase shifts in $\mathbf{q}$, $\dot{\mathbf{q}}$ and $\mathbf{i}$.
Joint accelerations $\ddot{\mathbf{q}}$ are derived from the filtered velocities
via numerical differentiation and, if filtering is enabled, are filtered
analogously.

\textbf{Subsampling by $K$.}
From the trajectory pool, we draw subsets of size
\[
  K \in \{8,\,16,\,32,\,48,\,64,\,86,\,122\},
\]
using three independent dataset seeds.
For each $(K,\mathrm{seed})$, a fixed trajectory split is created with
test fraction $0.2$ and validation fraction $0.1$ at the level of complete
trajectories (i.e., trajectories are never split across subsets).

\textbf{Stage~1 (DeLaN) configuration and model selection.}
For each $(K,\mathrm{seed})$ subset, we train five DeLaN initialisations
(seeds $s\in\{0,\dots,4\}$) and select the best DeLaN by validation error
(validation motor-current RMSE, equivalently MSE).
All DeLaN runs use the structured JAX implementation and a fixed hyperparameter
preset \texttt{lutter\_like\_256} (Table 1~\cite{Q4_2_lutter2023combiningphysicsdeeplearning}):
softplus activation, width $256$, depth $2$, mini-batch size $1024$,
learning rate $10^{-4}$ and weight decay $10^{-5}$.
Training is run for at most $200$ epochs with early stopping (patience $10$,
monitored on validation MSE) to avoid overfitting and to ensure that changes in
performance are attributable to $K$ rather than excessive training time.

\textbf{Stage~2 (LSTM) configuration.}
After selecting the best DeLaN, we freeze it and export residuals for each
trajectory.
Stage~2 uses a history length $H=100$ and feature mode \texttt{full}, i.e.,
each LSTM input time step concatenates
$(\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}},\hat{\mathbf{i}}_{\mathrm{DeLaN}})$.
The residual LSTM is trained with two stacked LSTM layers (units $128$), dropout
$0.2$, batch size $64$, and a validation split of $0.1$.
Training runs for at most $120$ epochs and employs early stopping on validation
loss (patience $20$, warm-up $10$ epochs), again fixing hyperparameters across
all $K$ to isolate the effect of trajectory quantity.

\begin{algorithm}[H]
\caption{K-domination experiment protocol for the DeLaN+LSTM pipeline}\label{alg:k_domination_protocol}
\begin{algorithmic}
\Require Trajectory pool $\mathcal{T}$ with $|\mathcal{T}|=122$ at $100\,\mathrm{Hz}$
\Require $K \in \{8,16,32,48,64,86,122\}$, dataset seeds $\mathcal{D}=\{0,1,2\}$
\Require DeLaN seeds $\mathcal{S}_{\mathrm{DeLaN}}=\{0,\dots,4\}$
\Ensure Aggregated learning curves and per-joint motor-current RMSE as a function of $K$

\State Low-pass filter each trajectory ($4^{\mathrm{th}}$-order Butterworth, $f_c=10\,\mathrm{Hz}$, zero-phase)
\ForAll{$K$}
    \ForAll{$d \in \mathcal{D}$}
        \State Sample $K$ trajectories from $\mathcal{T}$ using seed $d$
        \State Split into train/val/test trajectories (fixed fractions, no within-trajectory splitting)
	        \ForAll{$s \in \mathcal{S}_{\mathrm{DeLaN}}$}
	            \State Train DeLaN with fixed hyperparameters \Comment{max 200 epochs; early stop (patience 10)}
	            \State Record train loss and validation motor-current MSE/RMSE
	        \EndFor
	        \State Select best DeLaN by validation error and freeze its parameters
        \State Export residual motor currents per trajectory
        \State Build residual windows ($H=100$) and train LSTM \Comment{seed = dataset seed $d$; max 120 epochs; early stop (patience 20)}
        \State Record train/validation loss and residual motor-current RMSE
    \EndFor
    \State Aggregate across seeds: median $\pm$ IQR learning curves and progress-aligned errors
\EndFor
\end{algorithmic}
\end{algorithm}

\textbf{Aggregation and reporting.}
For each $K$, we aggregate results across the three dataset seeds.
Reported learning curves (train loss, validation loss) are shown as median
curves with interquartile ranges (IQR, $25$--$75$ percentile) across the 5 delan seeds per $(K,\mathrm{seed})$.
For Stage~1, where five DeLaN initialisations are trained per $(K,\mathrm{seed})$,
we first compute the median $\pm$ IQR across DeLaN seeds for each dataset seed,
and then aggregate these seed-wise median curves across dataset seeds.
For time-dependent error visualisations, trajectories are aligned by normalised
progress (mapping each trajectory time index to $[0,1]$) and resampled to a
fixed number of bins; median $\pm$ IQR is then computed per progress bin.

\subsection{Best Model Approch}

\textbf{Motivation.}
The K-domination sweep shows that once a sufficiently large number of trajectories is available,
the DeLaN training dynamics become comparable across $K$ and the remaining variance is dominated
by (i) which trajectories end up in the train/validation/test split (dataset seed) and (ii) the
network initialisation (DeLaN seed).
This effect is visible in the collapse of the median learning curves for $K\geq 32$
(Figures~\ref{fig:kdom_delan_train_loss_by_k} and~\ref{fig:kdom_delan_val_mse_by_k}), while the
dispersion and occasional outliers remain seed-dependent, especially for the progress-aligned
motor-current RMSE (Figure~\ref{fig:kdom_delan_rmse_progress_by_k}). \textbf{main objective best DelaN -> best LSTM here}

\textbf{Objective DeLaN}
To obtain a reliable basis for the second experiment, we therefore fix $K=84$ and select a DeLaN
hyperparameter preset within the stable $K$ regime.
Since Algorithm~\ref{alg:best_model_approach} performs validation-based checkpoint selection, the
hyperparameters should (i) reduce seed sensitivity (tight IQR with few or no catastrophic runs),
(ii) reach low validation error quickly and consistently (stable early-stopping behaviour), and
(iii) generalise such that the validation-based ranking correlates with test performance.
Accordingly, we do not select the setting with the lowest median validation error alone, but the
setting that achieves a favourable accuracy--robustness trade-off across dataset and initialisation
seeds.

\textbf{Robust DeLaN score for hyperparameter selection.}
For each DeLaN preset $h$ and dataset seed $d$, we aggregate validation RMSE across DeLaN seeds and compute a robustness-aware selection score
\begin{equation}
  \mathrm{score}(h,d)
  =
  \mathrm{median}\!\left(\mathrm{RMSE}_{\mathrm{val}}\right)
  + \lambda \cdot \mathrm{IQR}\!\left(\mathrm{RMSE}_{\mathrm{val}}\right)
  + P \cdot \rho(d,h),
\end{equation}
We use $\lambda=0.5$ and $P=10$ (in current units [A]).
Here, the median term captures typical validation accuracy, the IQR term penalises sensitivity to
dataset and initialisation seeds, and $\rho(d,h)$ denotes the divergence rate (fraction of failed runs under $h$ on dataset seed $d$, e.g., NaNs or missing metrics).
We then aggregate $\mathrm{score}(h,d)$ across dataset seeds and select the preset $h^\star$ with minimal median score.
Lower values of $\mathrm{score}(h,d)$ (and thus of its median across $d$) therefore indicate both high accuracy and high reliability.

\textbf{Aggregate scatter plots.}
In addition to the scalar score, we report two aggregated scatter plots for comparing presets $h$:
(i) median validation RMSE versus IQR of validation RMSE, and (ii) median validation RMSE versus
median test RMSE.
In both cases, statistics are computed across DeLaN seeds for each dataset seed first, and then
aggregated across dataset seeds, such that each point reflects the combined accuracy--robustness
behaviour under the dominant seed variability identified by K-domination.
The bottom-left region of the median--IQR plot corresponds to accurate and stable settings, while
the validation--test plot directly assesses whether validation error is a reliable selector for
test performance in the sense required by Algorithm~\ref{alg:best_model_approach}.

\textbf{DeLaN presets ($|\mathcal{H}_{\mathrm{DeLaN}}|=5$).}
Guided by the stable ``Lutter-like'' regime observed in the K-domination results for $K\geq 32$
(Figures~\ref{fig:kdom_delan_train_loss_by_k}--\ref{fig:kdom_delan_rmse_per_joint}), we evaluate the
following five presets at fixed $K=84$:
\begin{enumerate}
    \item \textbf{Baseline (Lutter-like):} SoftPlus, batch size $1024$, learning rate $10^{-4}$, weight decay $10^{-5}$, width $256$, depth $2$.
    \item \textbf{Smaller capacity:} baseline with width $128$, depth $2$.
    \item \textbf{Deeper network:} baseline with width $256$, depth $3$.
    \item \textbf{More regularisation:} baseline with weight decay $10^{-4}$.
    \item \textbf{Lower step size:} baseline with learning rate $5\cdot 10^{-5}$.
\end{enumerate}
This best-model study thus fixes $K=84$ and asks: \emph{within the stable regime}, which
hyperparameters provide the best accuracy--robustness trade-off across dataset splits and DeLaN
initialisations?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\begin{algorithm}[H]
\caption{Best-model selection for the DeLaN+LSTM pipeline}\label{alg:best_model_approach}
\begin{algorithmic}
\Require Trajectory pool size $K=84$ with fixed split $(N_{\mathrm{train}},N_{\mathrm{val}},N_{\mathrm{test}})=(59,8,17)$
\Require Dataset seeds $\mathcal{D}=\{0,1,2,3,4\}$, DeLaN seeds $\mathcal{S}_{\mathrm{DeLaN}}=\{0,1,2,3,4\}$, LSTM seeds $\mathcal{S}_{\mathrm{LSTM}}=\{0,1,2,3,4\}$
\Require Hyperparameter presets $\mathcal{H}_{\mathrm{DeLaN}}$ and $\mathcal{H}_{\mathrm{LSTM}}$ with $|\mathcal{H}_{\cdot}|=5$
\Ensure Best DeLaN checkpoint and best LSTM checkpoint for the fixed $K$

\State \textbf{Stage 1: DeLaN model selection}
\State Set $\lambda=0.5$, $P=10.0$ \Comment{stability weight, divergence penalty}
\ForAll{$h \in \mathcal{H}_{\mathrm{DeLaN}}$}
    \State Initialize fold summaries $\mathcal{F}_h \leftarrow \emptyset$
    \ForAll{$d \in \mathcal{D}$}
        \State Initialize run buffers $\mathcal{R} \leftarrow \emptyset$, curves $\mathcal{C}_{\mathrm{train}},\mathcal{C}_{\mathrm{val}} \leftarrow \emptyset$
	        \ForAll{$s \in \mathcal{S}_{\mathrm{DeLaN}}$}
	            \State Train DeLaN on fold $d$ with hyper-set $h$ and seed $s$ \Comment{early stopping on val MSE}
	            \If{training crashed or NaN/Inf or missing metrics}
	                \State Mark diverged and store placeholder metrics
	            \Else
	                \State Record curves $\mathrm{train\_loss}(e), \mathrm{val\_mse}(e)$, motor-current RMSE over progress, motor-current RMSE per joint, and $\mathrm{RMSE}_{\mathrm{val}}, \mathrm{RMSE}_{\mathrm{test}}, e^\star$       
	            \EndIf
	            \State Append curves and run metrics to $\mathcal{C}_{\mathrm{train}}, \mathcal{C}_{\mathrm{val}}, \mathcal{R}$ \Comment{includes motor-current RMSE progress and per-joint traces}
	        \EndFor
        \State Align curves to common length $E_{\max}$ and aggregate median$\,\pm\,$IQR
        \State Compute $\tilde{r}_{\mathrm{val}}(d,h)$, $q_{\mathrm{val}}(d,h)$, $\tilde{r}_{\mathrm{test}}(d,h)$ over non-diverged runs
        \State Compute divergence rate $\rho(d,h)=\#\mathrm{diverged}/|\mathcal{S}_{\mathrm{DeLaN}}|$
        \State Score $s(h,d)=\tilde{r}_{\mathrm{val}}(d,h)+\lambda\,q_{\mathrm{val}}(d,h)+P\,\rho(d,h)$
        \State Choose best seed $s^\star$ with minimal $\mathrm{RMSE}_{\mathrm{val}}$ among non-diverged runs
        \State Save fold artifacts for $(h,d)$ \Comment{median$\pm$IQR curves; best-run plots; checkpoint $(h,d,s^\star)$}
        \State Append fold summary $(\tilde{r}_{\mathrm{val}},q_{\mathrm{val}},\tilde{r}_{\mathrm{test}},\rho,s)$ to $\mathcal{F}_h$
    \EndFor
    \State Aggregate across dataset seeds for $h$:
    \Statex \hspace{1.25em}$S_h=\mathrm{median}_{d}\!\left(s(h,d)\right)$,\;
    $Q_h=\mathrm{IQR}_{d}\!\left(s(h,d)\right)$,\;
    $x_h=\mathrm{median}_{d}\!\left(\tilde{r}_{\mathrm{val}}(d,h)\right)$,\;
    $y^{\mathrm{test}}_h=\mathrm{median}_{d}\!\left(\tilde{r}_{\mathrm{test}}(d,h)\right)$
    \State Save hyper-set summary for $h$ and add one point to scatter datasets
\EndFor
\State Choose best hyper-set $h^\star=\arg\min_h S_h$
\State Plot scatter A: $x$ vs stability (median or IQR of $q_{\mathrm{val}}$ across $d$)
\State Plot scatter B: $x$ vs $y^{\mathrm{test}}_h$ \Comment{validation vs test alignment}
\State Choose final DeLaN checkpoint: $d^\star=\arg\min_d s(h^\star,d)$, use saved $(h^\star,d^\star,s^\star)$

\State \textbf{Freeze DeLaN and export residuals}
\State Freeze best DeLaN parameters and export residual motor currents per trajectory
\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{\textbf{Objective LSTM}}

\textbf{Motivation.}
After selecting and freezing the best DeLaN checkpoint (Algorithm~\ref{alg:best_model_approach}), Stage~2 addresses the remaining modelling error by learning residual motor currents.
For each time index $k$, the residual current is defined as
\begin{equation}
  \boldsymbol{r}_{i,k}
  =
  \mathbf{i}_{\mathrm{motor},k}
  -
  \hat{\mathbf{i}}_{\mathrm{DeLaN},k}.
  \label{eq:current_residual}
\end{equation}
In contrast to Stage~1, where architecture and training are kept fixed, the LSTM performance can depend strongly on the available history length $H$ and on which signals are provided as inputs.
We therefore treat the residual learner as a dedicated best-model selection problem under a fixed DeLaN baseline.

\textbf{Objective and selection criterion.}
The goal is to choose a configuration $(f,H)$ and corresponding checkpoints such that the \emph{end-to-end} prediction error of the combined motor-current model
\begin{equation}
  \hat{\mathbf{i}}_{\mathrm{RG},k}
  =
  \hat{\mathbf{i}}_{\mathrm{DeLaN},k}
  +
  \hat{\boldsymbol{r}}_{i,k}
  \label{eq:combined_current_rg}
\end{equation}
is minimised on held-out trajectories, while remaining robust to (i) the dataset split induced by the dataset seed $d$ and (ii) the LSTM initialisation seed.
Since the LSTM operates on history windows, evaluation is performed only on indices with valid context ($k\geq H-1$).

\textbf{Ablation axes.}
Algorithm~\ref{alg:lstm_best_model_approach} sweeps three window lengths $H\in\{50,100,150\}$ and four feature modes $f$ that probe whether residual dynamics are primarily explained by kinematics, by the DeLaN current baseline, or by their combination:
\texttt{state} uses $(\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}})$,
\texttt{i\_hat} uses $\hat{\mathbf{i}}_{\mathrm{DeLaN}}$,
\texttt{full} uses $(\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}},\hat{\mathbf{i}}_{\mathrm{DeLaN}})$, and
\texttt{state\_ihat} uses $(\dot{\mathbf{q}},\ddot{\mathbf{q}},\hat{\mathbf{i}}_{\mathrm{DeLaN}})$ (dropping $\mathbf{q}$).
For each $(d,f,H)$, multiple LSTM seeds are trained with early stopping, and both residual-level metrics and combined-model motor-current RMSE are recorded.

\textbf{Stability-aware ranking across seeds.}
To select a configuration that is both accurate and reliable, we rank $(f,H)$ by a robustness-aware score based on the combined-model motor-current RMSE:
for each dataset seed $d$, we aggregate across LSTM seeds using median and IQR, and add a divergence penalty for failed runs (NaN/Inf or missing metrics).
The resulting per-seed score $s(d,f,H)$ is then aggregated across dataset seeds, and the final choice is $(f^\star,H^\star)=\arg\min_{(f,H)} S_{f,H}$ as defined in Algorithm~\ref{alg:lstm_best_model_approach}.

\begin{algorithm}[H]
\caption{Best-model selection for the LSTM residual model}\label{alg:lstm_best_model_approach}
\begin{algorithmic}
\Require Dataset seeds $\mathcal{D}$, feature modes $\mathcal{F}=\{\texttt{full},\texttt{state},\texttt{i\_hat},\texttt{state\_ihat}\}$, window lengths $\mathcal{H}=\{50,100,150\}$
\Require LSTM seeds $\mathcal{S}_{\mathrm{LSTM}}$, stability weight $\lambda=0.5$, divergence penalty $P=10.0$
\Require Residual datasets exported from the best DeLaN for each $d \in \mathcal{D}$
\Ensure Best LSTM checkpoint and best $(f,H)$ configuration

\State \textbf{Build LSTM datasets (once per $(d,f,H)$)}
\ForAll{$d \in \mathcal{D}$}
    \State Load residual dataset for $d$
    \ForAll{$f \in \mathcal{F}$}
        \ForAll{$H \in \mathcal{H}$}
            \State Build LSTM window NPZ for $(d,f,H)$ \Comment{features by $f$, window length $H$}
        \EndFor
    \EndFor
\EndFor

\State \textbf{LSTM training and per-seed combined evaluation}
\ForAll{$d \in \mathcal{D}$}
    \ForAll{$f \in \mathcal{F}$}
        \ForAll{$H \in \mathcal{H}$}
            \State Initialize run buffers $\mathcal{R} \leftarrow \emptyset$, curves $\mathcal{C}_{\mathrm{train}},\mathcal{C}_{\mathrm{val}} \leftarrow \emptyset$
            \ForAll{$\ell \in \mathcal{S}_{\mathrm{LSTM}}$}
                \State Train LSTM on NPZ$(d,f,H)$ with seed $\ell$ \Comment{early stopping; save best checkpoint}
                \If{training crashed or NaN/Inf or missing metrics}
                    \State Mark diverged
                \Else
                    \State Record curves $\mathrm{train\_loss}(e), \mathrm{val\_loss}(e)$ and $e^\star$
                    \State Record residual metrics $\mathrm{RMSE}_{\mathrm{res}}, \mathrm{MSE}_{\mathrm{res}}$
                    \State Run combined evaluation with DeLaN baseline for $d$ and this LSTM
                    \State Record $\mathrm{RMSE}_{\mathrm{rg}}$, gain, gain\_ratio
                \EndIf
                \State Append curves and run metrics to $\mathcal{C}_{\mathrm{train}}, \mathcal{C}_{\mathrm{val}}, \mathcal{R}$
            \EndFor
            \State Align curves to common length $E_{\max}$ and aggregate median$\,\pm\,$IQR
            \State Compute medians and IQRs over non-diverged runs:
            \Statex \hspace{1.25em}$\tilde{r}_{\mathrm{res}}(d,f,H)$, $q_{\mathrm{res}}(d,f,H)$,
            $\tilde{r}_{\mathrm{rg}}(d,f,H)$, $q_{\mathrm{rg}}(d,f,H)$,
            $\tilde{g}(d,f,H)$, $q_g(d,f,H)$
            \State Compute divergence rate $\rho(d,f,H)=\#\mathrm{diverged}/|\mathcal{S}_{\mathrm{LSTM}}|$
            \State Score $s(d,f,H)=\tilde{r}_{\mathrm{rg}}(d,f,H)+\lambda\,q_{\mathrm{rg}}(d,f,H)+P\,\rho(d,f,H)$
            \State Save config artifacts for $(d,f,H)$ \Comment{median$\pm$IQR curves and metrics; score}
        \EndFor
    \EndFor
\EndFor

\State \textbf{Aggregate across dataset seeds (per $(f,H)$)}
\ForAll{$f \in \mathcal{F}$}
    \ForAll{$H \in \mathcal{H}$}
        \State Compute summary across $d$:
        \Statex \hspace{1.25em}$S_{f,H}=\mathrm{median}_{d}\!\left(s(d,f,H)\right)$,\;
        $Q_{f,H}=\mathrm{IQR}_{d}\!\left(s(d,f,H)\right)$,\;
        $\tilde{r}_{\mathrm{rg}}(f,H)=\mathrm{median}_{d}\!\left(\tilde{r}_{\mathrm{rg}}(d,f,H)\right)$,\;
        $\tilde{g}(f,H)=\mathrm{median}_{d}\!\left(\tilde{g}(d,f,H)\right)$
        \State Save per-$(f,H)$ summary and add to boxplots / progress-curve aggregation
    \EndFor
\EndFor

\State \textbf{Final selection}
\State Choose best $(f^\star,H^\star)=\arg\min_{(f,H)} S_{f,H}$
\State Report boxplots and progress-normalized RMSE curves for best configs
\end{algorithmic}
\end{algorithm}
