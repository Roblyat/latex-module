\chapter{Implementation}
This section details the practical implementation of the proposed approach, covering the simulation and real-world setup, datasets, software stack, and hardware configuration.

\section{Simulation Environment}
\begin{itemize}
    \item Evaluation of simulation frameworks for indoor semantic navigation:
    \begin{itemize}
        \item HabitatSim: Realistic Matterport3D-based environments with semantic annotations.
        \item Isaac Sim / Isaac Lab: GPU-accelerated simulation, advanced physics, support for RTX ray tracing.
        \item MuJoCo: High-speed physics engine, limited support for complex indoor scenes.
        \item Ignition Gazebo: Modular simulator, ROS2 integration, good for real-robot transfer.
    \end{itemize}
    \item \dots
\end{itemize}

\section{Dataset}
\begin{itemize}
    \item Use of \textbf{Matterport3D} scenes for realistic indoor environments with ground truth 3D reconstruction and semantic annotations.
    \item Incorporation of the \textbf{Habitat Navigation Challenge 2023} tasks to benchmark exploration and navigation performance (SR, SPL).
\end{itemize}

\section{Used Software}
\begin{itemize}
    \item ROS2-based implementation (Humble Hawksbill) as middleware.
    \item Navigation stack: Navigation2 (Nav2) for frontier-based exploration and path planning.
    \item DDS communication layer for distributed communication between detection, mapping, and control nodes.
    \item Custom RobotDriver for interfacing with real robot hardware.
    \item Integration of promptable models (SEEM, GroundingDINO, etc.) for real-time zero-shot detection during exploration and exploitation.
    \item \dots
\end{itemize}

\section{Used Hardware}
\begin{itemize}
    \item \textbf{PC:}
    \begin{itemize}
        \item CPU: AMD Ryzen 9 5950X 16-Core Processor
        \item Motherboard: B550 Gaming X V2
        \item GPU: ASUS TUF Gaming RTX 4090 24GB OC Edition
        \item RAM: 64GB Corsair Vengeance LPX DDR4
    \end{itemize}
    \item \textbf{Real Robot:} Configuration and components to be determined (tbd).
\end{itemize}

\section{Evaluation Metrics}
This section defines the evaluation metrics used throughout the experiments and assigns them to each corresponding experiment.

\begin{itemize}
    \item \textbf{Experiment 1 – Success Rate (SR):}  
    Measures the proportion of tasks in which the robot successfully reaches the queried single goal object. This metric reflects the system’s ability to semantically ground a user-specified object and to navigate toward it reliably. It serves as a fundamental indicator of task success and is essential for evaluating overall system effectiveness in basic search scenarios.
    \textit{Evaluation against:} \ac{VLFM}, \ac{VLMaps}, \ac{OneMap}, \ac{GeFF}

    \[
    \text{SR} = \frac{1}{N} \sum_{i=1}^{N} S_i
    \]
    where \( S_i = 1 \) if the goal was reached in episode \( i \), and \( 0 \) otherwise; \( N \) is the total number of episodes.


    \item \textbf{Experiment 2 – Path Efficiency (SPL):}  
    SPL measures the efficiency of successful navigation by comparing the shortest possible path to the actual path taken. It is defined only for successful runs and penalizes overly long trajectories. In the context of semantic exploration, SPL provides insight into how effectively the system prioritizes relevant regions and minimizes detours when searching for target objects.

    \[
    \text{SPL} = \frac{1}{N} \sum_{i=1}^{N} S_i \cdot \frac{l_i}{\max(p_i, l_i)}
    \]
    where \( S_i \) is the success indicator for episode \( i \), \( l_i \) is the shortest path length to the goal, \( p_i \) is the actual path length taken, and \( N \) is the total number of episodes.

    \item \textbf{Experiment 3 – Multi-Object Success Rate (MSR):}  
    The average number of successfully found objects per episode (\textit{Progress, PR}) captures partial success in multi-goal navigation. SPL is computed separately for each object in sequence, conditioned on the success of the previous one. This highlights the system’s ability to reuse semantic map information and improve efficiency across successive targets.

    \[
    \text{PR} = \frac{1}{N} \sum_{i=1}^{N} C_i
    \]
    where \( C_i \) is the number of successfully found objects in episode \( i \), and \( N \) is the total number of episodes.

    \item \textbf{Experiment 4 – Ablation: Memory Component (OpenFusion):}  
    Comparison of multi-object progress and SPL with and without the semantic 3D memory component. Highlights the contribution of global semantic mapping to task success and efficiency in the hybrid system.

    \item \textbf{Experiment 5 – Robustness to False Positives (Fusion Strategy):}  
    \textbf{Metrics:} Evaluated using semantic precision and false positive rate. Assesses the effect of the fusion strategy on filtering erroneous detections to improve overall task success.

    \[
    \text{Precision} = \frac{TP}{TP + FP}
    \quad\quad
    \text{FPR} = \frac{FP}{FP + TN}
    \]
    where \( TP \) and \( FP \) are the numbers of true and false positive semantic detections, and \( TN \) is the number of true negatives (i.e., correctly rejected background regions or non-target classes).

    \item \textbf{Experiment 6 – Real-World System Performance:}  
    \textbf{Metrics:}
    \begin{itemize}
        \item SR, MSR, SPL – for search performance under real-world conditions.
        \item System metrics – CPU/GPU usage, FPS, inference latency.
    \end{itemize}
    \textit{Objective: Assess robustness, efficiency, and deployability in physical environments.}

    \item \textbf{Experiment 7 – Vision-Language Model Comparison (SEEM vs. BLIP2):}  
    \textbf{Metrics:} Comparison of value maps generated using SEEM and BLIP2 for frontier scoring, focusing on their impact on exploration performance (SR and SPL) and GPU efficiency. Evaluates whether a lightweight unified model like SEEM can match or surpass BLIP2 in guiding semantic exploration through effective value map construction.
\end{itemize}