\chapter{Experimental and Results}
In \texttt{run\_sweep\_1.py}, training is organized as a controlled sweep over data regime and random seeds with (i) stabilized DeLaN training, (ii) explicit DeLaN model selection by validation error, and (iii) residual learning only for the selected baseline, followed by unified end-to-end evaluation.

\begin{itemize}
    \item \textbf{Outer sweep (data regime).} For each trajectory budget $K$ in the configured list (subsampled from the available training trajectories), DeLaN and LSTM training are executed under fixed hyperparameter regimes; in contrast to \texttt{run\_full\_sweep.py}, the epoch budgets are \emph{decoupled} from $K$.

    \item \textbf{Split sweep.} For each test fraction $t_f$ in the configured set (typically fixed to $t_f=0.2$) and validation fraction fixed to $v_f=0.1$, an independent experimental branch is executed.

    \item \textbf{Dataset replication.} For each dataset seed $s$, the raw CSV data is preprocessed into an NPZ dataset specific to $(K,t_f,v_f,s)$, defining train/validation/test partitions and derived signals (including acceleration filtering in the improved pipeline).

    \item \textbf{DeLaN replication and selection.} For each DeLaN seed $s_d$ in the configured set (e.g., $s_d\in\{0,1,2,3,4\}$), a structured DeLaN (JAX implementation) is trained on the same NPZ split using a fixed ``Lutter-like'' hyperparameter preset. The best DeLaN is then \emph{selected by validation torque RMSE} across $s_d$, and only this selected model is used for subsequent residual export.

    \item \textbf{Residual-model grid (restricted).} For the selected DeLaN, the script exports torque residuals and enumerates only window lengths $H \in \{100,150\}$ under a fixed feature mode (\texttt{full}). For each $H$, it (i) builds supervised LSTM windows, (ii) trains an LSTM residual predictor (with early stopping and best-checkpoint selection), and (iii) evaluates the recombined model (DeLaN + residual correction) using a unified evaluation routine, writing per-run metrics and plots.

    \item \textbf{Centralized summary logging.} In addition to per-run artifacts, the sweep appends DeLaN-, LSTM-, and end-to-end metrics to consolidated summary files (JSONL/CSV), enabling aggregate analysis across $(K,s,s_d,H)$ without directory scanning.
\end{itemize}

\paragraph{Summary of sweep-1 metrics (DeLaN, LSTM, and end-to-end evaluation).}
The sweep-1 experiment evaluates a physics-first baseline (DeLaN) and a learned residual corrector (LSTM) under a fixed feature configuration
(\texttt{full}) and two temporal window lengths ($H \in \{100,150\}$). For each dataset split (indexed by trajectory budget $K$ and dataset seed),
multiple DeLaN instances are trained with different DeLaN seeds; the best-performing DeLaN is selected by validation torque RMSE and subsequently
used to export residual targets for LSTM training. The reported metrics therefore reflect (i) DeLaN generalization as a function of data regime,
(ii) residual predictability under increasing data, and (iii) the compounded end-to-end performance when recombining baseline and residual
prediction.

\paragraph{DeLaN baseline: data-regime dominated generalization.}
Across all runs, DeLaN validation and test errors decrease strongly with increasing trajectory budget $K$. In the low-data regime (e.g., $K=8$),
DeLaN exhibits large errors and high variance across seeds, indicating that the learned inverse dynamics is not reliably identifiable under limited
excitation coverage. As $K$ increases, the baseline rapidly stabilizes: intermediate budgets (e.g., $K\approx 32$) reduce the error into a
substantially lower regime, while higher budgets (e.g., $K\ge 64$) yield tightly clustered test RMSE values. This behavior confirms that the
baseline physics learner is primarily limited by dataset richness rather than by optimizer noise, and that seed sensitivity is predominantly a
low-data phenomenon.

\paragraph{Residual learning (LSTM): improved predictability with larger $K$.}
The LSTM residual predictor shows the same overarching trend: residual test RMSE decreases markedly with increasing $K$, transitioning from a
difficult and unstable regime at very low $K$ to a low-variance, consistently learnable regime at higher $K$. Early stopping is effective in 
practice: the number of epochs run varies by difficulty, with challenging low-$K$ cases tending to train longer and high-$K$ cases converging
earlier. Comparing temporal contexts ($H=100$ versus $H=150$) reveals modest differences that are not strictly monotonic across budgets,
suggesting that both contexts are viable and that the optimal window length is data-dependent rather than universally fixed.

\paragraph{End-to-end recombination: substantial relative gains once the baseline is stable.}
The combined evaluation reports DeLaN baseline torque RMSE on the valid region ($k\ge H{-}1$) and the recombined model error after adding the
predicted residual. Since the recombined torque error satisfies
\[
\tau_{\mathrm{gt}} - \tau_{\mathrm{rg}} 
= \big(\tau_{\mathrm{gt}} - \tau_{\mathrm{DeLaN}}\big) - \hat{r}
= r_{\mathrm{gt}} - \hat{r},
\]
the recombined torque RMSE is numerically equivalent to the residual prediction RMSE under the evaluation protocol. Consequently, improvements in 
residual estimation translate directly into improvements in final torque prediction. In the high-data regimes,
the gain ratio $\mathrm{RMSE}_{\mathrm{rg}}/\mathrm{RMSE}_{\mathrm{DeLaN}}$ consistently falls well below one,
indicating strong relative improvement of the residual-corrected model over the physics baseline. Conversely,
in the low-data regime, gains are present but less reliable due to instability in both the baseline and the residual predictor.

\paragraph{Interpretation and implications for subsequent runs.}
Overall, the sweep-1 metrics support three conclusions. First, the dominant factor governing performance is the trajectory budget $K$: below a
minimum data regime the learned inverse dynamics is not consistently identifiable, whereas sufficiently large $K$ yields stable DeLaN baselines.
Second, residual learning provides consistent additional benefit once the baseline has entered the stable regime, demonstrating that the residual
component captures systematic structure not modeled by the physics network alone. Third, the remaining design choices examined here
(temporal context $H$ and additional $\ddot{q}$ lowpass toggling) have secondary influence relative to $K$ and do not dominate the observed performance.
These findings motivate concentrating subsequent compute budget on the higher-$K$ regimes and robust seed replication, while keeping the residual
model configuration fixed (\texttt{full} features, $H\in\{100,150\}$) to enable clean, interpretable comparisons.

\textbf{The DeLaN seed determines the character assignment between train and test set, which is comparable to a k-fold evaluation.}
Since the dataset situation, this is tested for this dataset in current precision [dataport dataset with paper Juan H.]
And on Torque Prediction for X != 6 dof

\paragraph{DeLaN baseline quality and generalization.}
For the selected DeLaN (chosen by minimum validation torque RMSE across DeLaN seeds), we first assess generalization behavior and qualitative fit. 
The training curve in Fig.~\ref{fig:delan_elbow_train_test} contrasts the optimization trajectory (training loss) against validation error and 
serves as an overfitting/underfitting sanity check for the chosen baseline. Qualitative agreement between predicted and ground-truth torques is 
then illustrated in Fig.~\ref{fig:delan_torque_vs_gt}, demonstrating that the physics model captures the dominant temporal structure of the inverse 
dynamics. Quantitatively, the remaining baseline error is summarized over time in Fig.~\ref{fig:delan_torque_rmse_time} and decomposed per joint in 
Fig.~\ref{fig:delan_torque_rmse_per_joint}. Together, these figures characterize both when (time-localized) and where (joint-localized) the 
baseline DeLaN remains limited, providing the reference point for residual correction.

\paragraph{Residual learnability (LSTM) and overfitting diagnostics.}
Given the exported residual target $r_{\mathrm{gt}}=\tau_{\mathrm{gt}}-\tau_{\mathrm{DeLaN}}$, the LSTM is trained to predict $\hat{r}$ from 
windowed inputs under the fixed \texttt{full} feature representation. The training dynamics are summarized by the loss curves in 
Fig.~\ref{fig:lstm_loss_curve}, where the evolution of training loss and validation loss motivates early stopping and best-checkpoint selection. 
A qualitative comparison of predicted and ground-truth residuals on the test split is shown in Fig.~\ref{fig:lstm_residual_gt_pred_test},
indicating that the model captures the dominant residual trends while leaving localized discrepancies during sharper transients.
The residual error is further summarized as a time-resolved average in Fig.~\ref{fig:lstm_residual_rmse_time} and as a per-joint decomposition
in Fig.~\ref{fig:lstm_residual_rmse_per_joint}. These plots establish that the residual is not purely noise but contains predictable structure
that can be learned consistently in the stable data regimes.

\paragraph{End-to-end recombination and torque improvement.}
The end-to-end effect of residual learning is evaluated by recombining the baseline and residual prediction via
$\tau_{\mathrm{rg}}=\tau_{\mathrm{DeLaN}}+\hat{r}$. Figure~\ref{fig:eval_torque_gt_delan_combined_test} provides the primary qualitative result: 
the recombined torque trace is visibly closer to the ground truth than the DeLaN baseline, particularly in segments where the baseline exhibits
systematic mismatch. This improvement is quantified over time in Fig.~\ref{fig:eval_torque_rmse_time_test}, and per joint in
Fig.~\ref{fig:eval_torque_rmse_per_joint_grouped}, demonstrating that gains are not uniformly distributed across joints but tend to align with 
joints exhibiting larger baseline error. Finally, Fig.~\ref{fig:eval_lstm_residual_gt_vs_pred_test} links the end-to-end torque improvement 
directly to residual predictability: since $\tau_{\mathrm{gt}}-\tau_{\mathrm{rg}} = r_{\mathrm{gt}}-\hat{r}$ under the evaluation protocol,
improvements in residual estimation translate one-to-one into improvements in final torque RMSE.
