\chapter{Methods}~\label{sec:methods}
Quick overview of what is found in methods (DeLaN + LSTM)-> May start with general DeLaN / LSTM explanation -> moves on with my pipeline mathematics -> then implementation of my pipeline, especially Dataset
and input-/output vector dimentions here, also dataset recording offline training online execution.

\section{Aim of Work}

The SoA review indicates that the strongest results for robot inverse dynamics and friction modelling are achieved when \emph{physics-structured} networks are combined with data-driven residual learners.
Extended DeLaN models can capture motor couplings and current-torque relations from encoder and motor data alone~\cite{Q4_1_extended_delan_motor,Q4_2_lutter2023combiningphysicsdeeplearning}, while recent
PINN-based approaches augment a structured dynamics model with temporal convolutions to obtain joint-torque prediction and friction compensation on industrial
robots~\cite{Q4_3_residual_pinns_dynamics_id,Q4_4_10729277}. At the same time, several works show that recurrent residual learners-in particular LSTMs-are highly effective for compensating modelling errors
and estimating joint torques and end-effector forces in closely related settings~\cite{Q3_1_tao_bll,Q3_3_lstm_force_estimation}.

The aim of this thesis is to build on these insights and develop a \emph{physics-informed, sequence-model-based inverse-dynamics architecture} that
can be trained using only encoder and motor-current data, yields an accurate nominal model of the robot-gripper joint torques and can be
consistently interpreted as a flange-wrench model in the force/torque sensor frame. Thereby providing a basis for tool/gripper compensation and subsequent
payload dynamic parameter identification (PDPI) as formulated in Section~\ref{sec:robot_dynamics_background}. Concretely, the work pursues the following objectives:
\begin{itemize}
\item \textbf{Stage~1 (structured inverse dynamics)} Stage~1 follows the improved DeLaN parameterisation of~\cite{Q4_6_HU2026103093}, using a Cholesky-factor inertia network and a learned potential,
    complemented by an explicit Coulomb-viscous joint-friction model~\cite{Q4_6_HU2026103093,QM_2_Coulomb_viscous_friction_ur5_10610737}. In contrast to the extended DeLaN-Motor architecture of~\cite{Q4_1_extended_delan_motor},
    which embeds detailed motor couplings and is trained directly in current space, we adopt a simplified joint-side formulation and supervise the model using motor torques~\ref{eq:tau_motor},
    obtained from encoder and motor-current data, where $k_t$ is give~\cite{QM_1_ur3_ur5_torque_constant}. The resulting DeLaN network is trained \emph{offline} on carefully designed excitation trajectories to
    learn the nominal robot-gripper inverse dynamics.

\item \textbf{Stage~2 (residual sequence model)} Stage~2 adopts the DeLaN + TCN paradigm of~\cite{Q4_4_10729277}: the structured inverse-dynamics model from Stage~1 provides
    a nominal torque prediction, and a separate sequence model (LSTM) learns residual joint torques from motion history.

\item \textbf{Evaluation in joint space and measurement frame.} Evaluate the resulting DeLaN+LSTM architecture both in joint space and in the force/torque sensor frame, quantifying its ability to reproduce the nominal
    robot-gripper wrench across diverse trajectories.
\end{itemize}

In this work, a 6D force/torque sensor is used primarily as a \emph{research instrument} to validate the joint-space modelling in the end-effector frame:
flange-wrench measurements $\vec{F}_{\mathrm{meas}}$ are compared against the wrenches obtained by mapping the joint-space DeLaN+LSTM predictions through the Jacobian.
Once consistency between predicted and measured flange wrenches has been demonstrated, the joint-space trained DeLaN+LSTM architecture provides a robust basis for PDPI by delivering a reliable tool/gripper
wrench compensation in the measurement frame.

By combining a physics-informed inverse-dynamics backbone with a sequence model trained offline but executed online, this thesis aims to move from the calibration-heavy, fragmented SoA towards a unified and
practically deployable notion of dynamic awareness: a cobot that can reliably predict its own joint torques, reproduce its nominal flange wrench in the measurement frame, and separate intrinsic
robot-gripper dynamics from payload/collision-induced effects.

\begin{itemize}
    \item[\textbf{RQ1}] 
        How accurately can a DeLaN-based inverse-dynamics model, trained purely in joint space, predict the flange wrench in the measurement frame once mapped through the Jacobian? 
        What extent does augmenting DeLaN with a residual sequence model improve this wrench prediction?
        % (Evaluation of DeLaN-only vs.\ DeLaN+sequence model, and whether Q4\_1/Q4\_4-style joint-space modelling remains robust when viewed in the measurement frame.)
    \item[\textbf{RQ2}]
        How does augmenting a DeLaN with a residual sequence model (LSTM) improve this wrench prediction?
    \item[\textbf{RQ3}]
        Does augmenting DeLaN with an LSTM-based residual model achieve joint-torque prediction and friction compensation performance comparable to a TCN-based residual model?                
        % (Evaluation of whether the simpler recurrent LSTM is as robust as a TCN residual in joint space.)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stage~1: Structured inverse dynamics for robot + fixed gripper}~\label{subsec:stage1}

In a first step, we learn a nominal inverse-dynamics model for the
robot-gripper system without payload and without contact.
From the synchronised dataset we obtain
\[
\bigl\{
  \mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k,
  \boldsymbol{I}_k,\vec{F}_{\mathrm{meas},k}
\bigr\}_{k=1}^{N},
\]
where $\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k \in \mathbb{R}^n$ are
joint position, velocity and acceleration, $\boldsymbol{I}_k \in \mathbb{R}^n$
are motor currents, and $\vec{F}_{\mathrm{meas},k} \in \mathbb{R}^6$ is the
measured flange wrench in the sensor frame $S$.
For brushless DC motors with torque constant $k_t$~\cite{QM_1_ur3_ur5_torque_constant}, the measured motor torques are
\begin{equation}
  \boldsymbol{\tau}_{\mathrm{motor},k} = k_t\,\boldsymbol{I}_k.
\end{equation}

The nominal robot-gripper dynamics are parameterised by a Deep Lagrangian
Network (DeLaN)~\cite{Q4_6_HU2026103093} with parameters $\boldsymbol{\theta}$ for the conservative
dynamics and $\boldsymbol{\psi}$ for friction and other non-conservative terms.
The network implements a Lagrangian
\begin{equation}
  \mathcal{L}_{\boldsymbol{\theta}}\bigl(\mathbf{q},\dot{\mathbf{q}}\bigr)
  =
  \tfrac{1}{2}\,\dot{\mathbf{q}}^{\top}
  \mathbf{M}_{\boldsymbol{\theta}}(\mathbf{q})\,\dot{\mathbf{q}}
  -
  V_{\boldsymbol{\theta}}(\mathbf{q}),
  \label{eq:lagrangian}
\end{equation}
with positive-definite inertia matrix $\mathbf{M}_{\boldsymbol{\theta}}(\mathbf{q})$
(e.g.\ represented via a Cholesky-factor network~(\ref{sec:app_inertia})) and potential
$V_{\boldsymbol{\theta}}(\mathbf{q})$~(\ref{sec:app_potential}) represented by a neural network.\cite{Q4_1_extended_delan_motor,Q4_4_10729277,Q4_6_HU2026103093}
Using the Euler-Lagrange equations yields the conservative joint torques
\begin{equation}
  \boldsymbol{\tau}_{\mathrm{cons}}
  (\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}};\boldsymbol{\theta})
  =
  \mathbf{M}_{\boldsymbol{\theta}}(\mathbf{q})\,\ddot{\mathbf{q}}
  +
  \mathbf{C}_{\boldsymbol{\theta}}(\mathbf{q},\dot{\mathbf{q}})\,\dot{\mathbf{q}}
  +
  \mathbf{G}_{\boldsymbol{\theta}}(\mathbf{q}),
\end{equation}
where $\mathbf{C}_{\boldsymbol{\theta}}$ and $\mathbf{G}_{\boldsymbol{\theta}}$ are
implicitly defined by $\mathcal{L}_{\boldsymbol{\theta}}$ \cite{Q4_4_10729277,Q4_6_HU2026103093}.

Joint friction and other non-conservative effects are captured by a
Coulomb-viscous friction model, following the improved DeLaN design
of~\cite{Q4_6_HU2026103093,QM_2_Coulomb_viscous_friction_ur5_10610737}.
For each joint $i$ we adopt
\begin{equation}
  \tau_{\mathrm{fric},i}(\dot q_i;\boldsymbol{\psi})
  =
  f_{c,i}\,\operatorname{sgn}(\dot q_i)
  +
  f_{v,i}\,\dot q_i,
\end{equation}
with learned Coulomb and viscous coefficients $f_{c,i}$ and $f_{v,i}$,
collected in $\boldsymbol{\psi}$.
In vector form this can be written compactly as
\begin{equation}
  \boldsymbol{\tau}_{\mathrm{fric}}(\dot{\mathbf{q}};\boldsymbol{\psi})
  =
  f_{\mathrm{fric}}\bigl([\dot{\mathbf{q}},\operatorname{sgn}(\dot{\mathbf{q}})];\boldsymbol{\psi}\bigr),
\end{equation}
where $f_{\mathrm{fric}}$ denotes the joint-wise affine map implementing
the Coulomb-viscous law~(\ref{sec:app_friction}).

The DeLaN torque prediction is the sum of conservative and frictional parts,
\begin{equation}
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN}}
  (\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}};\boldsymbol{\theta},\boldsymbol{\psi})
  =
  \boldsymbol{\tau}_{\mathrm{cons}}
    (\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}};\boldsymbol{\theta})
  +
  \boldsymbol{\tau}_{\mathrm{fric}}(\dot{\mathbf{q}};\boldsymbol{\psi}).
\end{equation}
For compactness we write this as a parametric model
\begin{equation}
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN}}
  (\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}};\boldsymbol{\theta},\boldsymbol{\psi})
  =
  f_{\mathrm{DeLaN}}\bigl(\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}};\boldsymbol{\theta},\boldsymbol{\psi}\bigr),
\end{equation}
so that for sample $k$
\begin{equation}
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k}
  =
  f_{\mathrm{DeLaN}}\bigl(
    \mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k;
    \boldsymbol{\theta},\boldsymbol{\psi}
  \bigr).
\end{equation}

The parameters $(\boldsymbol{\theta},\boldsymbol{\psi})$ are trained offline by
minimising a joint-space regression loss~\cite{Q4_4_10729277,Q4_6_HU2026103093}
\begin{equation}
  \mathcal{L}_{\mathrm{DeLaN}}(\boldsymbol{\theta},\boldsymbol{\psi})
  =
  \frac{1}{N}\sum_{k=1}^{N}
  \left\|
    \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN}}
      (\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k;
       \boldsymbol{\theta},\boldsymbol{\psi})
    -
    \boldsymbol{\tau}_{\mathrm{motor},k}
  \right\|_2^2.
  \label{eq:delan_joint_loss}
\end{equation}
Note that this training objective uses only joint states and motor torques; the
force/torque sensor is not required for fitting the DeLaN model.

After training, the DeLaN parameters are frozen and the model serves as a
data-driven nominal inverse-dynamics model of the robot-gripper system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stage~2: Sequence model for residual joint torques}~\label{subsec:stage2}

In the second stage, we model history-dependent effects that are not captured
by the structured DeLaN model, such as backlash and fine-grained nonlinear
friction. Using the same robot-gripper dataset (still without payload and
without contact), we first compute the joint-space residual torques
\begin{equation}
  \boldsymbol{r}_{\tau,k}
  =
  \boldsymbol{\tau}_{\mathrm{motor},k}
  -
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k}.
  \label{eq:tau_residual}
\end{equation}

Let $H$ denote the sequence length (number of time steps in the history
window). For each time index $k \geq H$ we construct an input sequence
\begin{equation}
  \mathbf{x}_k
  =
  \Bigl[
    \mathbf{q}_{k-H+1:k},\,
    \dot{\mathbf{q}}_{k-H+1:k},\,
    \ddot{\mathbf{q}}_{k-H+1:k},\,
    \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k-H+1:k}
  \Bigr],
\end{equation}
where $\mathbf{q}_{a:b}$ denotes the stacked joint vectors
$(\mathbf{q}_a,\dots,\mathbf{q}_b)$, and analogously for
$\dot{\mathbf{q}}$, $\ddot{\mathbf{q}}$ and
$\hat{\boldsymbol{\tau}}_{\mathrm{DeLaN}}$.
This construction stacks the last $H$ joint states together with the
corresponding DeLaN torque predictions into a single sequence feature vector $\mathbf{x}_k$.

An LSTM with parameters $\boldsymbol{\varphi}$ maps this sequence to a
residual-torque prediction
\begin{equation}
  \hat{\boldsymbol{r}}_{\tau,k}
  =
  f_{\mathrm{LSTM}}(\mathbf{x}_k;\boldsymbol{\varphi})
  \in \mathbb{R}^n.
\end{equation}
The LSTM is trained to minimise the mean-squared error between predicted and
true residual torques,
\begin{equation}
  \mathcal{L}_{\mathrm{LSTM}}(\boldsymbol{\varphi})
  =
  \frac{1}{N_H}
  \sum_{k=H}^{N}
  \left\|
    \hat{\boldsymbol{r}}_{\tau,k}
    -
    \boldsymbol{r}_{\tau,k}
  \right\|_2^2,
  \label{eq:lstm_loss}
\end{equation}
where $N_H = N-H+1$ is the number of valid sequences.

The combined joint-space model for the robot-gripper system is then
\begin{equation}
  \hat{\boldsymbol{\tau}}_{\mathrm{RG},k}
  =
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k}
  +
  \hat{\boldsymbol{r}}_{\tau,k}.
  \label{eq:combined_torque_rg}
\end{equation}
For evaluation in the sensor frame, the corresponding combined flange wrench is
obtained via the Jacobian mapping
\begin{equation}
  \hat{\vec{F}}_{\mathrm{RG},k}
  =
  {}^{S}\!J(\mathbf{q}_k)^{-\top}\,
  \hat{\boldsymbol{\tau}}_{\mathrm{RG},k}.
  \label{eq:combined_wrench_rg}
\end{equation}
Since both stages are trained exclusively on data without payload and without
environment contact, $\hat{\boldsymbol{\tau}}_{\mathrm{RG},k}$ and
$\hat{\vec{F}}_{\mathrm{RG},k}$ represent a high-fidelity, history-aware model
of the nominal robot-gripper dynamics. In later stages, deviations between
this model and the measured joint torques or flange wrenches can be attributed
to the effective rigid-body contribution of additional payloads and contacts.

\begin{figure}[H]
\centering
\includegraphics[width=1\columnwidth]{Images/3_methods/251209_pipeline.drawio.png}
\caption{Two-stage learning pipeline for the nominal robot-gripper dynamics.
Stage~1 learns a structured inverse-dynamics model (DeLaN) in joint space from
encoder and motor-current data and is trained by regressing motor torques.
In Stage~2, a recurrent sequence model (LSTM) takes joint-state histories and
DeLaN torque predictions as input and learns residual joint torques over a
fixed history window. The combined joint-space model is then mapped through the
Jacobian to obtain the nominal flange wrench in the force/torque sensor frame,
which is compared against the measured wrench for evaluation.}
~\label{fig:methods_pipeline}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data preprocessing and dataset construction}
\label{sec:data_preprocessing}

The starting point is a long-format log of robot measurements, exported as a CSV
with columns
\[
  \texttt{Time},\;
  \texttt{Joint Name},\;
  \texttt{Position},\;
  \texttt{Velocity},\;
  \texttt{Acceleration},\;
  \texttt{Effort}.
\]
Each row corresponds to a single joint at a given timestamp. In this first
iteration, the joint velocities and accelerations are taken directly from the
controller without additional filtering, and the joint effort is interpreted as
a torque-like quantity. The precise mapping from motor currents to joint
torques is introduced later via the torque constant $k_t$
(cf.~Section~\ref{sec:methods}).

\paragraph{Joint selection (UR5 arm).}
From the full log we select only the six UR5 joints
\[
  \mathcal{J}
  =
  \bigl\{
    \texttt{ur5\_shoulder\_pan\_joint},\,
    \texttt{ur5\_shoulder\_lift\_joint},\,
    \texttt{ur5\_elbow\_joint},\,
    \texttt{ur5\_wrist\_1\_joint},\,
    \texttt{ur5\_wrist\_2\_joint},\,
    \texttt{ur5\_wrist\_3\_joint}
  \bigr\},
\]
which defines $n_{\mathrm{dof}} = 6$ for all subsequent processing. For each
timestamp $t$ we thus obtain a 6-dimensional joint configuration, velocity,
acceleration and effort vector.

\paragraph{Frame-wise representation and trajectory segmentation.}
Let $\{t_f\}_{f=1}^{F}$ denote the sorted set of unique timestamps in the log.
For each frame index $f$ and each joint $j \in \mathcal{J}$ we collect
\[
  q_{f,j},\quad
  \dot q_{f,j},\quad
  \ddot q_{f,j},\quad
  \tau^{\mathrm{eff}}_{f,j},
\]
where $\tau^{\mathrm{eff}}_{f,j}$ denotes the measured joint effort.
Stacking over joints yields frame-wise vectors
\[
  \mathbf{q}_f,\ \dot{\mathbf{q}}_f,\ \ddot{\mathbf{q}}_f,\ 
  \boldsymbol{\tau}^{\mathrm{eff}}_f \in \mathbb{R}^{6}.
\]

To obtain multiple trajectories from a single continuous log, we segment the
frames into fixed-length windows. For a chosen number of frames per trajectory,
$T_{\mathrm{seg}}$, we define a trajectory index
\[
  \mathrm{traj}(f)
  =
  \left\lfloor \frac{f-1}{T_{\mathrm{seg}}} \right\rfloor,
\]
and group all frames with the same $\mathrm{traj}(f)$ into one trajectory.

For trajectory $i$ this yields a time series of length $T_i$ with
\begin{align}
  \mathbf{t}^{(i)} &= \bigl(t^{(i)}_1,\dots,t^{(i)}_{T_i}\bigr) \in \mathbb{R}^{T_i}, \\
  \mathbf{Q}^{(i)} &= 
    \begin{bmatrix}
      \mathbf{q}^{(i)}_1{}^\top \\
      \vdots \\
      \mathbf{q}^{(i)}_{T_i}{}^\top
    \end{bmatrix}
    \in \mathbb{R}^{T_i \times 6}, \\
  \dot{\mathbf{Q}}^{(i)} &=
    \begin{bmatrix}
      \dot{\mathbf{q}}^{(i)}_1{}^\top \\
      \vdots \\
      \dot{\mathbf{q}}^{(i)}_{T_i}{}^\top
    \end{bmatrix}
    \in \mathbb{R}^{T_i \times 6}, \\
  \ddot{\mathbf{Q}}^{(i)} &=
    \begin{bmatrix}
      \ddot{\mathbf{q}}^{(i)}_1{}^\top \\
      \vdots \\
      \ddot{\mathbf{q}}^{(i)}_{T_i}{}^\top
    \end{bmatrix}
    \in \mathbb{R}^{T_i \times 6}, \\
  \mathbf{T}^{(i)} &=
    \begin{bmatrix}
      \boldsymbol{\tau}^{\mathrm{eff},(i)}_1{}^\top \\
      \vdots \\
      \boldsymbol{\tau}^{\mathrm{eff},(i)}_{T_i}{}^\top
    \end{bmatrix}
    \in \mathbb{R}^{T_i \times 6}.
\end{align}
In short, each trajectory $i$ is represented by
\[
  \bigl(\mathbf{t}^{(i)},\mathbf{Q}^{(i)},\dot{\mathbf{Q}}^{(i)},
        \ddot{\mathbf{Q}}^{(i)},\mathbf{T}^{(i)}\bigr),
\]
with shapes
\[
  \mathbf{t}^{(i)} \in \mathbb{R}^{T_i},\quad
  \mathbf{Q}^{(i)},\dot{\mathbf{Q}}^{(i)},\ddot{\mathbf{Q}}^{(i)},\mathbf{T}^{(i)}
  \in \mathbb{R}^{T_i \times 6}.
\]

\paragraph{Train/test split by trajectory.}
Let $\mathcal{I} = \{1,\dots,N_{\mathrm{traj}}\}$ denote the set of all
trajectory indices.
We randomly permute $\mathcal{I}$ and partition it into disjoint training and
test index sets,
\[
  \mathcal{I}_{\mathrm{train}} \cup \mathcal{I}_{\mathrm{test}} = \mathcal{I},
  \qquad
  \mathcal{I}_{\mathrm{train}} \cap \mathcal{I}_{\mathrm{test}} = \emptyset,
\]
such that a given trajectory is entirely in either the training or the test
set. This avoids leakage of near-identical neighbouring samples across splits.

\paragraph{NPZ dataset structure.}
For efficient loading during model training, the trajectory-wise data are stored
in a NumPy \texttt{.npz} archive with the following keys:
\begin{align}
  &\texttt{train\_labels},\ 
   \texttt{train\_t},\ 
   \texttt{train\_q},\ 
   \texttt{train\_qd},\ 
   \texttt{train\_qdd},\ 
   \texttt{train\_tau}, \\
  &\texttt{test\_labels},\ 
   \texttt{test\_t},\ 
   \texttt{test\_q},\ 
   \texttt{test\_qd},\ 
   \texttt{test\_qdd},\ 
   \texttt{test\_tau}.
\end{align}
Each of the trajectory-wise arrays is stored as a one-dimensional object array.
For example,
\[
  \texttt{train\_q}[i]
  \;\widehat{=}\;
  \mathbf{Q}^{(i)} \in \mathbb{R}^{T_i \times 6},
\]
and analogously for $\texttt{train\_qd}$, $\texttt{train\_qdd}$ and
$\texttt{train\_tau}$.
Thus, if there are $N_{\mathrm{train}}$ training trajectories, we have
\[
  \texttt{train\_q} \in \mathbb{R}^{N_{\mathrm{train}}}_{\text{object}},\quad
  \texttt{train\_q}[i] \in \mathbb{R}^{T_i \times 6},
\]
and likewise for the test set.

In the current implementation the \texttt{Effort} column is used to populate
$\mathbf{T}^{(i)}$; the conversion to motor torques via
$\boldsymbol{\tau}_{\mathrm{motor},k} = k_t\,\boldsymbol{I}_k$ is applied
later in the training pipeline (cf.~\eqref{eq:tau_motor}).

\paragraph{Flattening trajectories for DeLaN training.}
On the DeLaN side, the trajectory-wise arrays are flattened into a single pool
of samples for stochastic optimisation.
Denoting concatenation along the time dimension by $\mathrm{vstack}(\cdot)$,
the training set becomes
\begin{align}
  \tilde{\mathbf{Q}}_{\mathrm{train}}
    &= \mathrm{vstack}\bigl(\texttt{train\_q}[i]\bigr)
     \in \mathbb{R}^{N_{\mathrm{train,tot}} \times 6}, \\
  \dot{\tilde{\mathbf{Q}}}_{\mathrm{train}}
    &= \mathrm{vstack}\bigl(\texttt{train\_qd}[i]\bigr)
     \in \mathbb{R}^{N_{\mathrm{train,tot}} \times 6}, \\
  \ddot{\tilde{\mathbf{Q}}}_{\mathrm{train}}
    &= \mathrm{vstack}\bigl(\texttt{train\_qdd}[i]\bigr)
     \in \mathbb{R}^{N_{\mathrm{train,tot}} \times 6}, \\
  \tilde{\mathbf{T}}_{\mathrm{train}}
    &= \mathrm{vstack}\bigl(\texttt{train\_tau}[i]\bigr)
     \in \mathbb{R}^{N_{\mathrm{train,tot}} \times 6},
\end{align}
where
\[
  N_{\mathrm{train,tot}} = \sum_{i \in \mathcal{I}_{\mathrm{train}}} T_i
\]
is the total number of training time steps across all trajectories.
An analogous flattening is performed for the test set.

All arrays are cast to a floating-point dtype (e.g.\ \texttt{float32}) to
avoid \texttt{dtype=object} issues in the JAX-based DeLaN implementation.

\vspace{0.75\baselineskip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{DeLaN training setup}
\label{sec:delan_training_setup}

Given the flattened dataset
\[
  \mathcal{D}_{\mathrm{train}}
  =
  \bigl\{
    (\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k,
     \boldsymbol{\tau}_{\mathrm{motor},k})
  \bigr\}_{k=1}^{N_{\mathrm{train,tot}}},
\]
the DeLaN model $f_{\mathrm{DeLaN}}$ is trained as described in
Section~\ref{sec:methods}. Here we summarise the main implementation details
used in this work.

\paragraph{Feature transform on joint positions.}
Instead of feeding the raw joint angles $\mathbf{q}_k$ directly into the
DeLaN subnetworks, we use a bounded, trigonometric feature map
\begin{equation}
  \boldsymbol{\phi}(\mathbf{q}_k)
  =
  \begin{bmatrix}
    \mathbf{q}_k \\
    \sin(\mathbf{q}_k) \\
    \cos(\mathbf{q}_k)
  \end{bmatrix}
  \in \mathbb{R}^{3n_{\mathrm{dof}}},
\end{equation}
where the sine and cosine are applied element-wise.
This follows the common practice in DeLaN/PINN-based models of encoding
revolute joints through periodic features, which mitigates angle wrap-around
and keeps the network inputs well scaled.
The feature vector $\boldsymbol{\phi}(\mathbf{q}_k)$ is used as input to the
inertia MLPs and the potential network described in
Appendix~\ref{ch:appendix_delan}.

\paragraph{Per-joint loss normalisation.}
To prevent joints with large torque magnitudes from dominating the optimisation,
we employ a per-joint normalisation in the inverse-dynamics loss.
Let
\begin{equation}
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k}
  =
  f_{\mathrm{DeLaN}}
    (\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k;
     \boldsymbol{\theta},\boldsymbol{\psi})
\end{equation}
denote the DeLaN torque prediction for sample $k$.

We compute empirical variances of the training torques
\begin{equation}
  \boldsymbol{\sigma}^2_{\tau}
  =
  \mathrm{Var}\bigl(
    \tilde{\mathbf{T}}_{\mathrm{train}}
  \bigr)
  \in \mathbb{R}^{6},
\end{equation}

and define a diagonal weighting matrix

\begin{equation}
  \mathbf{W}_{\tau}
  =
  \mathrm{diag}\bigl( \boldsymbol{\sigma}^{-1}_{\tau} \bigr)
  \in \mathbb{R}^{6 \times 6},
\end{equation}
where the inverse is taken element-wise.
The training objective can then be written as
\begin{equation}
  \mathcal{L}_{\mathrm{DeLaN}}(\boldsymbol{\theta},\boldsymbol{\psi})
  =
  \frac{1}{N_{\mathrm{train,tot}}}
  \sum_{k=1}^{N_{\mathrm{train,tot}}}
  \bigl\|
    \mathbf{W}_{\tau}
    \bigl(
      \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k}
      -
      \boldsymbol{\tau}_{\mathrm{motor},k}
    \bigr)
  \bigr\|_2^2,
\end{equation}
which corresponds to a per-joint normalisation of the squared torque error.
In practice, additional factors derived from $\ddot{\mathbf{q}}$ may be
included analogously, but the essential idea is that the loss is balanced
across joints.

\paragraph{Replay buffer and random mini-batch sampling.}
All training samples are stored in a replay buffer $\mathcal{M}$, which
supports random-access mini-batch sampling.
At each optimisation step, a mini-batch index set
$S \subset \{1,\dots,N_{\mathrm{train,tot}}\}$ with $|S| = B$ is drawn (e.g.\
by taking a random permutation of indices and slicing), and the corresponding
batch
\[
  \mathcal{B}
  =
  \bigl\{
    (\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k,
     \boldsymbol{\tau}_{\mathrm{motor},k})
  \bigr\}_{k \in S}
\]
is used to evaluate $\mathcal{L}_{\mathrm{DeLaN}}$ and its gradients.
This renders the optimisation effectively i.i.d.\ over the flattened pool of
samples; the original trajectory grouping is retained only for the train/test
split and for later sequence-based models and visualisations.

\paragraph{Importance of constant sampling time.}
Throughout the preprocessing and training pipeline, we assume a (approximately)
constant sampling interval
\[
  \Delta t_k = t_{k+1} - t_k \approx \Delta t,\qquad \forall k,
\]
as is standard for robot control logs.
A fixed sampling period is crucial for:
\begin{itemize}
  \item interpreting measured joint velocities $\dot{\mathbf{q}}_k$ and
        accelerations $\ddot{\mathbf{q}}_k$ as consistent derivatives of
        $\mathbf{q}_k$,
  \item applying finite-difference schemes or filtering to reconstruct
        $\dot{\mathbf{q}}_k$ and $\ddot{\mathbf{q}}_k$ from position data,
  \item ensuring that the DeLaN model learns a time-homogeneous mapping from
        $(\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k)$ to
        $\boldsymbol{\tau}_{\mathrm{motor},k}$.
\end{itemize}
In the present work, the raw controller velocities and accelerations are used
directly, but enforcing a constant sampling time and applying dedicated
filtering to $\dot{\mathbf{q}}_k$ and $\ddot{\mathbf{q}}_k$ constitutes a
straightforward extension of the preprocessing pipeline.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Long-Short-Term-Memory Training Setup}

\vspace{0.75\baselineskip}

\textbf{Export DeLaN Residuals}

After training Stage~1, the DeLaN parameters $(\boldsymbol{\theta},\boldsymbol{\psi})$
are frozen and the model is evaluated on all training samples
$(\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k)$,
$k=1,\dots,N_{\mathrm{train,tot}}$.
This yields joint-torque predictions
\begin{equation}
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k}
  =
  f_{\mathrm{DeLaN}}
    (\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k;
     \boldsymbol{\theta},\boldsymbol{\psi}),
\end{equation}
which we stack into a matrix
\begin{equation}
  \hat{\mathbf{T}}_{\mathrm{DeLaN,train}}
  =
  \begin{bmatrix}
    \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},1}{}^\top \\
    \vdots \\
    \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},N_{\mathrm{train,tot}}}{}^\top
  \end{bmatrix}
  \in \mathbb{R}^{N_{\mathrm{train,tot}} \times 6}.
\end{equation}

Using the corresponding motor torques
$\boldsymbol{\tau}_{\mathrm{motor},k}$, we define the joint-space residual
torques for all training samples as
\begin{equation}
  \boldsymbol{r}_{\tau,k}
  =
  \boldsymbol{\tau}_{\mathrm{motor},k}
  -
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k},
  \qquad k = 1,\dots,N_{\mathrm{train,tot}},
\end{equation}
and collect them in
\begin{equation}
  \mathbf{R}_{\tau,\mathrm{train}}
  =
  \begin{bmatrix}
    \boldsymbol{r}_{\tau,1}{}^\top \\
    \vdots \\
    \boldsymbol{r}_{\tau,N_{\mathrm{train,tot}}}{}^\top
  \end{bmatrix}
  \in \mathbb{R}^{N_{\mathrm{train,tot}} \times 6}.
\end{equation}

\vspace{0.75\baselineskip}

\textbf{Build LSTM Windows}

Let $H$ denote the history length (sequence length) used in Stage~2.
From the flattened arrays
$\tilde{\mathbf{Q}}_{\mathrm{train}},
 \dot{\tilde{\mathbf{Q}}}_{\mathrm{train}},
 \ddot{\tilde{\mathbf{Q}}}_{\mathrm{train}},
 \hat{\mathbf{T}}_{\mathrm{DeLaN,train}}$
we then construct overlapping sequences with a sliding window.
For each time index $k \geq H$ we define the LSTM input sequence
\begin{equation}
  \mathbf{x}_k
  =
  \Bigl[
    \mathbf{q}_{k-H+1:k},\,
    \dot{\mathbf{q}}_{k-H+1:k},\,
    \ddot{\mathbf{q}}_{k-H+1:k},\,
    \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k-H+1:k}
  \Bigr],
\end{equation}
where, as in the main Methods section,
$\mathbf{q}_{a:b}$ denotes the stacked joint vectors
$(\mathbf{q}_a,\dots,\mathbf{q}_b)$ and analogously for
$\dot{\mathbf{q}}$, $\ddot{\mathbf{q}}$ and
$\hat{\boldsymbol{\tau}}_{\mathrm{DeLaN}}$.
The corresponding target for each sequence is chosen as the residual torque
at the final time step,
\begin{equation}
  \mathbf{y}_k = \boldsymbol{r}_{\tau,k} \in \mathbb{R}^6.
\end{equation}

Stacking all valid windows yields the Stage~2 training dataset
\begin{align}
  \mathbf{X}^{\mathrm{LSTM}}_{\mathrm{train}}
    &= \bigl\{\mathbf{x}_k\bigr\}_{k=H}^{N_{\mathrm{train,tot}}}, \\
  \mathbf{Y}^{\mathrm{LSTM}}_{\mathrm{train}}
    &= \bigl\{\mathbf{y}_k\bigr\}_{k=H}^{N_{\mathrm{train,tot}}},
\end{align}
with $N_H = N_{\mathrm{train,tot}} - H + 1$ sequences in total.
In implementation, $\mathbf{X}^{\mathrm{LSTM}}_{\mathrm{train}}$ is stored as
a tensor of shape $(N_H, H, d_{\mathrm{in}})$, where
$d_{\mathrm{in}} = 4n_{\mathrm{dof}}$ corresponds to
$(\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}},\hat{\boldsymbol{\tau}}_{\mathrm{DeLaN}})$
per time step, and
$\mathbf{Y}^{\mathrm{LSTM}}_{\mathrm{train}} \in \mathbb{R}^{N_H \times 6}$.
A completely analogous construction is used for the test split.

\vspace{0.75\baselineskip}

\textbf{LSTM Training}

\vspace{0.75\baselineskip}

We train an LSTM-based residual model $g_{\boldsymbol{\phi}}$ on the windowed
dataset $(\mathbf{x}_k,\mathbf{y}_k)$.
As in Stage~1, the goal is to prevent single joints from dominating the
objective. To this end, we apply a train-only standardisation of both inputs
and targets, and train the network to predict \emph{scaled} residuals.

\paragraph{Train-only standardisation.}
Let
$\mathbf{X}^{\mathrm{LSTM}}_{\mathrm{train}} \in \mathbb{R}^{N_H \times H \times d_{\mathrm{in}}}$
and
$\mathbf{Y}^{\mathrm{LSTM}}_{\mathrm{train}} \in \mathbb{R}^{N_H \times 6}$
denote the windowed training set.
We compute feature-wise input statistics across all windows and time steps,
\begin{equation}
  \mu_{x,j}
  =
  \frac{1}{N_H H}
  \sum_{k=1}^{N_H}\sum_{t=1}^{H} X^{\mathrm{LSTM}}_{\mathrm{train}}[k,t,j],
  \qquad
  \sigma^2_{x,j}
  =
  \frac{1}{N_H H}
  \sum_{k=1}^{N_H}\sum_{t=1}^{H}
  \bigl(X^{\mathrm{LSTM}}_{\mathrm{train}}[k,t,j] - \mu_{x,j}\bigr)^2,
\end{equation}
for $j=1,\dots,d_{\mathrm{in}}$, and collect them into vectors
$\boldsymbol{\mu}_x,\boldsymbol{\sigma}_x \in \mathbb{R}^{d_{\mathrm{in}}}$.
Analogously, we compute per-joint target statistics over windows,
\begin{equation}
  \mu_{y,i}
  =
  \frac{1}{N_H}
  \sum_{k=1}^{N_H} Y^{\mathrm{LSTM}}_{\mathrm{train}}[k,i],
  \qquad
  \sigma^2_{y,i}
  =
  \frac{1}{N_H}
  \sum_{k=1}^{N_H}
  \bigl(Y^{\mathrm{LSTM}}_{\mathrm{train}}[k,i] - \mu_{y,i}\bigr)^2,
\end{equation}
for $i=1,\dots,6$, yielding
$\boldsymbol{\mu}_y,\boldsymbol{\sigma}_y \in \mathbb{R}^{6}$.
To avoid numerical issues, standard deviations below a small threshold
$\varepsilon$ are clamped to $1$ component-wise.
Defining diagonal scaling matrices
\[
  \mathbf{W}_x = \mathrm{diag}(\boldsymbol{\sigma}_x^{-1}),
  \qquad
  \mathbf{W}_y = \mathrm{diag}(\boldsymbol{\sigma}_y^{-1}),
\]
the standardised inputs and targets are then given by
\begin{align}
  \mathbf{x}^{\mathrm{n}}_{k,t}
    &= \mathbf{W}_x \bigl(\mathbf{x}_{k,t} - \boldsymbol{\mu}_x\bigr), \\
  \boldsymbol{r}^{\mathrm{s}}_{\tau,k}
    &= \mathbf{W}_y \bigl(\boldsymbol{r}_{\tau,k} - \boldsymbol{\mu}_y\bigr),
\end{align}
with $\boldsymbol{r}_{\tau,k}=\mathbf{y}_k$.
The same $(\boldsymbol{\mu}_x,\boldsymbol{\sigma}_x,\boldsymbol{\mu}_y,\boldsymbol{\sigma}_y)$
computed on the training split are used to scale the test split.

\paragraph{Architecture and objective.}
The residual model is implemented as a two-layer LSTM with dropout regularisation
and a linear output layer,
\begin{equation}
  \hat{\boldsymbol{r}}_{\tau,k}^{\mathrm{s}}
  =
  g_{\boldsymbol{\phi}}
  \bigl(
    \mathbf{x}^{\mathrm{n}}_k
  \bigr)
  \in \mathbb{R}^{6},
\end{equation}
where $\mathbf{x}^{\mathrm{n}}_k$ denotes the standardised input window and
$\hat{\boldsymbol{r}}_{\tau,k}^{\mathrm{s}}$ the predicted \emph{scaled} residual.
Training minimises a mean-squared error on scaled residuals,
\begin{equation}
  \mathcal{L}_{\mathrm{LSTM}}(\boldsymbol{\phi})
  =
  \frac{1}{N_H}
  \sum_{k=1}^{N_H}
  \left\|
    \hat{\boldsymbol{r}}_{\tau,k}^{\mathrm{s}}
    -
    \boldsymbol{r}_{\tau,k}^{\mathrm{s}}
  \right\|_2^2,
\end{equation}
which corresponds to a balanced, per-joint normalisation via
$\boldsymbol{\sigma}_y$.
Optimisation is performed with Adam using shuffled mini-batches and an internal
validation split; the best-performing model is selected by early stopping on
the validation loss.

\paragraph{Inference (unscaled units).}
For reporting and for composing the final DeLaN+LSTM torque prediction, the
scaled residual is mapped back to physical units via
\begin{equation}
  \hat{\boldsymbol{r}}_{\tau,k}
  =
  \mathbf{W}_y^{-1}\hat{\boldsymbol{r}}_{\tau,k}^{\mathrm{s}}
  +
  \boldsymbol{\mu}_y,
\end{equation}
where $\mathbf{W}_y^{-1}=\mathrm{diag}(\boldsymbol{\sigma}_y)$.
