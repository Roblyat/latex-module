\chapter{Methods}~\label{sec:methods}
This chapter formalises the proposed two-stage inverse-dynamics pipeline and its learning objectives.
It first introduces the physics-inspired DeLaN backbone and the residual LSTM correction model, and then derives the combined predictor used throughout the thesis.
Finally, the chapter specifies the dataset interface in terms of input/output signal vectors and dimensions, and clarifies the offline training versus online execution setting adopted in the experimental pipeline.
To motivate the perspective of this work towards gripper/tool manipulation and compensation, as well as payload dynamic parameter identification, the structured inverse-dynamics formulation is also related to the end-effector wrench measurement frame.
However, this thesis evaluates the developed method purely in joint space via motor-current modelling.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stage~1: Structured inverse dynamics for robot + fixed gripper}~\label{subsec:stage1}

In a first step, we learn a nominal inverse-dynamics model for the
robot-gripper system without payload and without contact.
From the synchronised dataset we obtain
\[
\bigl\{
  \mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k,
  \boldsymbol{I}_k,\vec{F}_{\mathrm{meas},k}
\bigr\}_{k=1}^{N},
\]
where $\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k \in \mathbb{R}^n$ are
joint position, velocity and acceleration, $\boldsymbol{I}_k \in \mathbb{R}^n$
are motor currents, and $\vec{F}_{\mathrm{meas},k} \in \mathbb{R}^6$ is the
measured flange wrench in the sensor frame $S$.
For brushless DC motors with torque constant $k_t$~\cite{QM_1_ur3_ur5_torque_constant}, the measured motor torques, for the main area of operation
range are
\begin{equation}
  \boldsymbol{\tau}_{\mathrm{motor},k} = k_t\,\boldsymbol{I}_k.
\end{equation}

The nominal robot-gripper dynamics are parameterised by a Deep Lagrangian
Network (DeLaN)~\cite{Q4_6_HU2026103093} with parameters $\boldsymbol{\theta}$ for the conservative
dynamics and $\boldsymbol{\psi}$ for friction and other non-conservative terms.
The network implements a Lagrangian
\begin{equation}
  \mathcal{L}_{\boldsymbol{\theta}}\bigl(\mathbf{q},\dot{\mathbf{q}}\bigr)
  =
  \tfrac{1}{2}\,\dot{\mathbf{q}}^{\top}
  \mathbf{M}_{\boldsymbol{\theta}}(\mathbf{q})\,\dot{\mathbf{q}}
  -
  V_{\boldsymbol{\theta}}(\mathbf{q}),
  \label{eq:lagrangian}
\end{equation}
with positive-definite inertia matrix $\mathbf{M}_{\boldsymbol{\theta}}(\mathbf{q})$
(e.g.\ represented via a Cholesky-factor network) and potential
$V_{\boldsymbol{\theta}}(\mathbf{q})$ represented by a neural network.\cite{Q4_2_lutter2023combiningphysicsdeeplearning, Q4_1_extended_delan_motor,Q4_4_10729277,Q4_6_HU2026103093}
Using the Euler-Lagrange equations yields the conservative joint torques
\begin{equation}
  \boldsymbol{\tau}_{\mathrm{cons}}
  (\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}};\boldsymbol{\theta})
  =
  \mathbf{M}_{\boldsymbol{\theta}}(\mathbf{q})\,\ddot{\mathbf{q}}
  +
  \mathbf{C}_{\boldsymbol{\theta}}(\mathbf{q},\dot{\mathbf{q}})\,\dot{\mathbf{q}}
  +
  \mathbf{G}_{\boldsymbol{\theta}}(\mathbf{q}),
\end{equation}
where $\mathbf{C}_{\boldsymbol{\theta}}$ and $\mathbf{G}_{\boldsymbol{\theta}}$ are
implicitly defined by $\mathcal{L}_{\boldsymbol{\theta}}$ \cite{Q4_4_10729277,Q4_6_HU2026103093}.

Joint friction and other non-conservative effects, for example joint backlash or friction and heat losses, are captured by Long-Short-Term-Memory Network further described in Subsection~\ref{subsec:stage2},
Section~\ref{seq:LSTM_Training}

The DeLaN torque prediction is the are the conservative parts of the inverse dynamic system,
so that for sample $k$

\begin{equation}
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN}}
  (\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}};\boldsymbol{\theta})
  =
  \boldsymbol{\tau}_{\mathrm{cons}}
    (\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}};\boldsymbol{\theta}).
\end{equation}

The parameter $\boldsymbol{\theta}$ is trained offline by
minimising a joint-space regression loss~\cite{Q4_2_lutter2023combiningphysicsdeeplearning}
\begin{equation}
  \mathcal{L}_{\mathrm{DeLaN}}(\boldsymbol{\theta},\boldsymbol{\psi})
  =
  \frac{1}{N}\sum_{k=1}^{N}
  \left\|
    \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN}}
      (\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k;
       \boldsymbol{\theta})
    -
    \boldsymbol{\tau}_{\mathrm{motor},k}
  \right\|_2^2.
  \label{eq:delan_joint_loss}
\end{equation}
Note that this training objective uses only joint states and motor torques. The
force/torque sensor is not required for fitting the DeLaN model.

After training, the DeLaN parameters are frozen and the model serves as a
data-driven nominal inverse-dynamics model of the robot-gripper system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stage~2: Sequence model for residual joint torques}~\label{subsec:stage2}

In the second stage, we model history-dependent effects that are not captured
by the structured DeLaN model, such as backlash and nonlinear
friction. Using the same robot-gripper dataset (still without payload and
without contact), we first compute the joint-space residual torques
\begin{equation}
  \boldsymbol{r}_{\tau,k}
  =
  \boldsymbol{\tau}_{\mathrm{motor},k}
  -
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k}.
  \label{eq:tau_residual}
\end{equation}

Let $H$ denote the sequence length (number of time steps in the history
window). For each time index $k \geq H$ we construct an input sequence
\begin{equation}
  \mathbf{x}_k
  =
  \Bigl[
    \mathbf{q}_{k-H+1:k},\,
    \dot{\mathbf{q}}_{k-H+1:k},\,
    \ddot{\mathbf{q}}_{k-H+1:k},\,
    \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k-H+1:k}
  \Bigr],
\end{equation}
where $\mathbf{q}_{a:b}$ denotes the stacked joint vectors
$(\mathbf{q}_a,\dots,\mathbf{q}_b)$, and analogously for
$\dot{\mathbf{q}}$, $\ddot{\mathbf{q}}$ and
$\hat{\boldsymbol{\tau}}_{\mathrm{DeLaN}}$.
This construction stacks the last $H$ joint states together with the
corresponding DeLaN torque predictions into a single sequence feature vector $\mathbf{x}_k$.

An LSTM with parameters $\boldsymbol{\varphi}$ maps this sequence to a
residual-torque prediction
\begin{equation}
  \hat{\boldsymbol{r}}_{\tau,k}
  =
  f_{\mathrm{LSTM}}(\mathbf{x}_k;\boldsymbol{\varphi})
  \in \mathbb{R}^n.
\end{equation}
The LSTM is trained to minimise the mean-squared error between predicted and
true residual torques,
\begin{equation}
  \mathcal{L}_{\mathrm{LSTM}}(\boldsymbol{\varphi})
  =
  \frac{1}{N_H}
  \sum_{k=H}^{N}
  \left\|
    \hat{\boldsymbol{r}}_{\tau,k}
    -
    \boldsymbol{r}_{\tau,k}
  \right\|_2^2,
  \label{eq:lstm_loss}
\end{equation}
where $N_H = N-H+1$ is the number of valid sequences.

The combined joint-space model for the robot-gripper system is then
\begin{equation}
  \hat{\boldsymbol{\tau}}_{\mathrm{RG},k}
  =
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k}
  +
  \hat{\boldsymbol{r}}_{\tau,k}.
  \label{eq:combined_torque_rg}
\end{equation}
For evaluation in the sensor frame, the corresponding combined flange wrench is
obtained via the Jacobian mapping
\begin{equation}
  \hat{\vec{F}}_{\mathrm{RG},k}
  =
  {}^{S}\!J(\mathbf{q}_k)^{-\top}\,
  \hat{\boldsymbol{\tau}}_{\mathrm{RG},k}.
  \label{eq:combined_wrench_rg}
\end{equation}
Since both stages are trained exclusively on data without payload and without
environment contact, $\hat{\boldsymbol{\tau}}_{\mathrm{RG},k}$ and
$\hat{\vec{F}}_{\mathrm{RG},k}$ represent a high-fidelity, history-aware model
of the nominal robot-gripper dynamics. In later stages, deviations between
this model and the measured joint torques or flange wrenches can be attributed
to the effective rigid-body contribution of additional payloads and contacts.

\begin{figure}[H]
\centering
\includegraphics[width=1\columnwidth]{Images/3_methods/251209_pipeline.drawio.png}
\caption{Two-stage learning pipeline for the nominal robot-gripper dynamics.
Stage~1 learns a structured inverse-dynamics model (DeLaN) in joint space from
encoder and motor-current data and is trained by regressing motor torques.
In Stage~2, a recurrent sequence model (LSTM) takes joint-state histories and
DeLaN torque predictions as input and learns residual joint torques over a
fixed history window. The combined joint-space model is then mapped through the
Jacobian to obtain the nominal flange wrench in the force/torque sensor frame,
which is compared against the measured wrench for evaluation.}
~\label{fig:methods_pipeline}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Dataset of Collaborative Robots}

All experiments are based on the publicly available ``Dataset of Collaborative Robots for
Energy Consumption Modeling'' released via IEEE DataPort~\cite{Dataset_Dataport} and
documented in~\cite{Q_Dataset_Paper}.
The dataset contains measurements from two Universal Robots platforms (UR3e and UR10e)
recorded both without load and with an external payload.

Each log sample provides a time stamp $t$ and a trajectory identifier, and includes joint-space
signals (joint positions $\mathbf{q}$, joint velocities $\dot{\mathbf{q}}$) together with electrical
measurements (per-joint motor currents $\mathbf{i}$, motor voltages, as well as
robot-level current and voltage).
In addition, the dataset provides end-effector quantities such as Cartesian position and the
measured wrench (force and moment) at the end effector.
In the remainder of this thesis, motor current is treated as the central measured actuation signal.

To excite the robot dynamics across a wide range of operating conditions, the robots execute
sinusoidal joint motions with varying amplitudes, frequencies, and initial conditions~\cite{Q_Dataset_Paper}:
\begin{equation}
  q_i(t) = q_{i0} + A_i \cos\!\bigl(2\pi f_i t + \varphi_i\bigr),
\end{equation}
where $q_i$, $q_{i0}$, $A_i$, $f_i$ and $\varphi_i$ denote the desired joint position, initial
position, amplitude, oscillation frequency, and phase of joint $i$, respectively.
The experiments were conducted for UR3e and UR10e under two load conditions: without load
and with an attached payload (hammer $1.5\,\mathrm{kg}$ and RobotiQ 2F-85 gripper $1\,\mathrm{kg}$)~\cite{Q_Dataset_Paper}.

The signals are recorded at $100\,\mathrm{Hz}$.
For each robot (and load condition), the dataset provides $50{,}000$ samples for training and
$5{,}000$ samples for testing~\cite{Q_Dataset_Paper}.
Since the underlying dynamics are time-invariant, the published dataset is formed by combining
multiple shorter recordings into one consistent dataset, without treating discontinuities as
separate experiments~\cite{Q_Dataset_Paper}.

Since the dataset provides motor current measurements, we formulate training objectives and
evaluation metrics in the measured actuation domain as per-joint motor currents
$\mathbf{i}_{\mathrm{motor}}$ in $\mathrm{A}$.
When needed for interpretation, current-domain errors can be mapped to equivalent motor-domain
errors via the (previously introduced) constant conversion factor $k_t$, by multiplying with
$k_t$.
Consistent with the pipeline overview in Fig.~\ref{fig:methods_pipeline}, we omit the conversion
$\boldsymbol{\tau}_{\mathrm{motor}} = k_t\,\mathbf{i}_{\mathrm{motor}}$ and the rightmost Jacobian-based
flange-wrench mapping block, and evaluate the approach directly as a combined motor-current model
$\hat{\mathbf{i}}_{\mathrm{comb}}$.

\section{Data preprocessing and dataset construction}
\label{sec:data_preprocessing}

The preprocessing step converts the raw IEEE DataPort logs into a DeLaN-ready,
trajectory-wise dataset.
In our implementation, we use a \emph{wide} CSV representation in which each row
corresponds to one time step and contains all six joint channels, together with
an explicit trajectory identifier.
Concretely, the wide format provides a time stamp \texttt{t1}, per-joint
kinematics \texttt{q1}--\texttt{q6} and \texttt{dq1}--\texttt{dq6}, per-joint
motor currents \texttt{Iq1}--\texttt{Iq6}, and an integer \texttt{ID} column.
The DeLaN preprocessing service maps this wide table into the long-format
interface expected by the upstream codebase\cite{Q4_2_lutter2023combiningphysicsdeeplearning} (columns \texttt{Time}, \texttt{Joint Name}, \texttt{Position}, \texttt{Velocity}, \texttt{Acceleration}, \texttt{Effort}),
propagating \texttt{ID} as \texttt{trajectory\_id}.
Thus, trajectory boundaries are taken directly from the dataset rather than
being inferred by heuristic re-segmentation.

We consider the six revolute joints of the manipulator and use a fixed ordering
$j\in\{1,\dots,6\}$ consistent with the wide columns.
For each row (frame) index $f$ we read
\[
  t_f,\quad
  q_{f,j},\quad
  \dot q_{f,j},\quad
  I_{f,j},
\]
where $I_{f,j}$ denotes the measured motor current.
Stacking over joints yields the frame-wise vectors
\[
  \mathbf{q}_f,\ \dot{\mathbf{q}}_f,\ \mathbf{I}_f \in \mathbb{R}^{6}.
\]
For compatibility with the DeLaN data interface, the current vector is stored
as a generic actuation channel $\boldsymbol{\tau}^{\mathrm{eff}}_f := \mathbf{I}_f$.

Let $\mathcal{I}=\{1,\dots,N_{\mathrm{traj}}\}$ denote the set of unique
trajectory identifiers.
For each $i\in\mathcal{I}$, we collect all frames with \texttt{trajectory\_id}
equal to $i$, sort them by time, and stack them into a variable-length
trajectory of length $T_i$.
To attenuate sensor noise while avoiding temporal misalignment, we apply a
4th-order Butterworth low-pass filter with cutoff frequency $f_c = 10\,\mathrm{Hz}$.
The filter is applied in zero-phase form (forward--backward filtering), thereby
preventing phase shifts in $\mathbf{q}$, $\dot{\mathbf{q}}$ and $\mathbf{I}$.
Joint accelerations $\ddot{\mathbf{q}}$ are then derived from the filtered
velocities via numerical differentiation and are filtered analogously, which
aligns with the baseline preprocessing used for the dataset~\cite{Q_Dataset_Paper}.

For trajectory $i$ this yields a time series of length $T_i$ with
\begin{align}
  \mathbf{t}^{(i)} &= \bigl(t^{(i)}_1,\dots,t^{(i)}_{T_i}\bigr) \in \mathbb{R}^{T_i}, \\
  \mathbf{Q}^{(i)} &= 
    \begin{bmatrix}
      \mathbf{q}^{(i)}_1{}^\top \\
      \vdots \\
      \mathbf{q}^{(i)}_{T_i}{}^\top
    \end{bmatrix}
    \in \mathbb{R}^{T_i \times 6}, \\
  \dot{\mathbf{Q}}^{(i)} &=
    \begin{bmatrix}
      \dot{\mathbf{q}}^{(i)}_1{}^\top \\
      \vdots \\
      \dot{\mathbf{q}}^{(i)}_{T_i}{}^\top
    \end{bmatrix}
    \in \mathbb{R}^{T_i \times 6}, \\
  \ddot{\mathbf{Q}}^{(i)} &=
    \begin{bmatrix}
      \ddot{\mathbf{q}}^{(i)}_1{}^\top \\
      \vdots \\
      \ddot{\mathbf{q}}^{(i)}_{T_i}{}^\top
    \end{bmatrix}
    \in \mathbb{R}^{T_i \times 6}, \\
  \mathbf{T}^{(i)} &=
    \begin{bmatrix}
      \boldsymbol{\tau}^{\mathrm{eff},(i)}_1{}^\top \\
      \vdots \\
      \boldsymbol{\tau}^{\mathrm{eff},(i)}_{T_i}{}^\top
    \end{bmatrix}
    \in \mathbb{R}^{T_i \times 6}.
\end{align}
In short, each trajectory $i$ is represented by
\[
  \bigl(\mathbf{t}^{(i)},\mathbf{Q}^{(i)},\dot{\mathbf{Q}}^{(i)},
        \ddot{\mathbf{Q}}^{(i)},\mathbf{T}^{(i)}\bigr),
\]
with shapes
\[
  \mathbf{t}^{(i)} \in \mathbb{R}^{T_i},\quad
  \mathbf{Q}^{(i)},\dot{\mathbf{Q}}^{(i)},\ddot{\mathbf{Q}}^{(i)},\mathbf{T}^{(i)}
  \in \mathbb{R}^{T_i \times 6}.
\]

We randomly permute $\mathcal{I}$ (with a dataset seed) and partition it into
disjoint training and test index sets,
\[
  \mathcal{I}_{\mathrm{train}} \cup \mathcal{I}_{\mathrm{test}} = \mathcal{I},
  \qquad
  \mathcal{I}_{\mathrm{train}} \cap \mathcal{I}_{\mathrm{test}} = \emptyset,
\]
such that a given trajectory is entirely in either the training or the test
set. This avoids leakage of near-identical neighbouring samples across splits.
For the learning-curve experiments, we optionally subsample a fixed number of
trajectories $K$ (seeded) before splitting, such that each $(K,\mathrm{seed})$
corresponds to a fixed trajectory subset.

For efficient loading during model training, the trajectory-wise data are stored
in a NumPy \texttt{.npz} archive with the following keys:
\begin{align}
  &\texttt{train\_labels},\ 
   \texttt{train\_t},\ 
   \texttt{train\_q},\ 
   \texttt{train\_qd},\ 
   \texttt{train\_qdd},\ 
   \texttt{train\_tau}, \\
  &\texttt{test\_labels},\ 
   \texttt{test\_t},\ 
   \texttt{test\_q},\ 
   \texttt{test\_qd},\ 
   \texttt{test\_qdd},\ 
   \texttt{test\_tau}.
\end{align}
Each of the trajectory-wise arrays is stored as a one-dimensional object array.
For example,
\[
  \texttt{train\_q}[i]
  \;\widehat{=}\;
  \mathbf{Q}^{(i)} \in \mathbb{R}^{T_i \times 6},
\]
and analogously for $\texttt{train\_qd}$, $\texttt{train\_qdd}$ and
$\texttt{train\_tau}$.
Thus, if there are $N_{\mathrm{train}}$ training trajectories, we have
\[
  \texttt{train\_q} \in \mathbb{R}^{N_{\mathrm{train}}}_{\text{object}},\quad
  \texttt{train\_q}[i] \in \mathbb{R}^{T_i \times 6},
\]
and likewise for the test set.

In the current implementation the \texttt{Effort} column is used to populate
$\mathbf{T}^{(i)}$; the conversion to motor torques via
$\boldsymbol{\tau}_{\mathrm{motor},k} = k_t\,\boldsymbol{I}_k$ is applied
later in the training pipeline (cf.~\eqref{eq:tau_motor}).

On the DeLaN side, the trajectory-wise arrays are flattened into a single pool
of samples for stochastic optimisation.
Denoting concatenation along the time dimension by $\mathrm{vstack}(\cdot)$,
the training set becomes
\begin{align}
  \tilde{\mathbf{Q}}_{\mathrm{train}}
    &= \mathrm{vstack}\bigl(\texttt{train\_q}[i]\bigr)
     \in \mathbb{R}^{N_{\mathrm{train,tot}} \times 6}, \\
  \dot{\tilde{\mathbf{Q}}}_{\mathrm{train}}
    &= \mathrm{vstack}\bigl(\texttt{train\_qd}[i]\bigr)
     \in \mathbb{R}^{N_{\mathrm{train,tot}} \times 6}, \\
  \ddot{\tilde{\mathbf{Q}}}_{\mathrm{train}}
    &= \mathrm{vstack}\bigl(\texttt{train\_qdd}[i]\bigr)
     \in \mathbb{R}^{N_{\mathrm{train,tot}} \times 6}, \\
  \tilde{\mathbf{T}}_{\mathrm{train}}
    &= \mathrm{vstack}\bigl(\texttt{train\_tau}[i]\bigr)
     \in \mathbb{R}^{N_{\mathrm{train,tot}} \times 6},
\end{align}
where
\[
  N_{\mathrm{train,tot}} = \sum_{i \in \mathcal{I}_{\mathrm{train}}} T_i
\]
is the total number of training time steps across all trajectories.
An analogous flattening is performed for the test set.

All arrays are cast to a floating-point dtype (e.g.\ \texttt{float32}) to
avoid \texttt{dtype=object} issues in the JAX-based DeLaN implementation.

\vspace{0.75\baselineskip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{DeLaN training setup}
\label{sec:delan_training_setup}

Given the flattened dataset
\[
  \mathcal{D}_{\mathrm{train}}
  =
  \bigl\{
    (\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k,
     \boldsymbol{\tau}_{\mathrm{motor},k})
  \bigr\}_{k=1}^{N_{\mathrm{train,tot}}},
\]
the DeLaN model $f_{\mathrm{DeLaN}}$ is trained as described in
Section~\ref{sec:methods}. Here we summarise the main implementation details
used in this work.

Instead of feeding the raw joint angles $\mathbf{q}_k$ directly into the
DeLaN subnetworks, we use a bounded, trigonometric feature map
\begin{equation}
  \boldsymbol{\phi}(\mathbf{q}_k)
  =
  \begin{bmatrix}
    \mathbf{q}_k \\
    \sin(\mathbf{q}_k) \\
    \cos(\mathbf{q}_k)
  \end{bmatrix}
  \in \mathbb{R}^{3n_{\mathrm{dof}}},
\end{equation}
where the sine and cosine are applied element-wise.
This follows the common practice in DeLaN/PINN-based models of encoding
revolute joints through periodic features, which mitigates angle wrap-around
and keeps the network inputs well scaled.

To prevent joints with large torque magnitudes from dominating the optimisation,
we employ a per-joint normalisation in the inverse-dynamics loss.
Let
\begin{equation}
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k}
  =
  f_{\mathrm{DeLaN}}
    (\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k;
     \boldsymbol{\theta},\boldsymbol{\psi})
\end{equation}
denote the DeLaN torque prediction for sample $k$.

We compute empirical variances of the training torques
\begin{equation}
  \boldsymbol{\sigma}^2_{\tau}
  =
  \mathrm{Var}\bigl(
    \tilde{\mathbf{T}}_{\mathrm{train}}
  \bigr)
  \in \mathbb{R}^{6},
\end{equation}

and define a diagonal weighting matrix

\begin{equation}
  \mathbf{W}_{\tau}
  =
  \mathrm{diag}\bigl( \boldsymbol{\sigma}^{-1}_{\tau} \bigr)
  \in \mathbb{R}^{6 \times 6},
\end{equation}
where the inverse is taken element-wise.
The training objective can then be written as
\begin{equation}
  \mathcal{L}_{\mathrm{DeLaN}}(\boldsymbol{\theta},\boldsymbol{\psi})
  =
  \frac{1}{N_{\mathrm{train,tot}}}
  \sum_{k=1}^{N_{\mathrm{train,tot}}}
  \bigl\|
    \mathbf{W}_{\tau}
    \bigl(
      \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k}
      -
      \boldsymbol{\tau}_{\mathrm{motor},k}
    \bigr)
  \bigr\|_2^2,
\end{equation}
which corresponds to a per-joint normalisation of the squared torque error.
In practice, additional factors derived from $\ddot{\mathbf{q}}$ may be
included analogously, but the essential idea is that the loss is balanced
across joints.

All training samples are stored in a replay buffer $\mathcal{M}$, which
supports random-access mini-batch sampling.
At each optimisation step, a mini-batch index set
$S \subset \{1,\dots,N_{\mathrm{train,tot}}\}$ with $|S| = B$ is drawn (e.g.\
by taking a random permutation of indices and slicing), and the corresponding
batch
\[
  \mathcal{B}
  =
  \bigl\{
    (\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k,
     \boldsymbol{\tau}_{\mathrm{motor},k})
  \bigr\}_{k \in S}
\]
is used to evaluate $\mathcal{L}_{\mathrm{DeLaN}}$ and its gradients.
This renders the optimisation effectively i.i.d.\ over the flattened pool of
samples. The original trajectory grouping is retained only for the train/test
split and for later sequence-based models and visualisations.

Throughout the preprocessing and training pipeline, we assume a (approximately)
constant sampling interval
\[
  \Delta t_k = t_{k+1} - t_k \approx \Delta t,\qquad \forall k,
\]
as is standard for robot control logs.
A fixed sampling period is crucial for:
\begin{itemize}
  \item interpreting measured joint velocities $\dot{\mathbf{q}}_k$ and
        accelerations $\ddot{\mathbf{q}}_k$ as consistent derivatives of
        $\mathbf{q}_k$,
  \item applying finite-difference schemes or filtering to reconstruct
        $\dot{\mathbf{q}}_k$ and $\ddot{\mathbf{q}}_k$ from position data,
  \item ensuring that the DeLaN model learns a time-homogeneous mapping from
        $(\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k)$ to
        $\boldsymbol{\tau}_{\mathrm{motor},k}$.
\end{itemize}
In the present work, the raw controller velocities and accelerations are used
directly, but enforcing a constant sampling time and applying dedicated
filtering to $\dot{\mathbf{q}}_k$ and $\ddot{\mathbf{q}}_k$ constitutes a
straightforward extension of the preprocessing pipeline.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Long-Short-Term-Memory Training Setup}~\label{seq:LSTM_Training}

\vspace{0.75\baselineskip}

After training Stage~1, the DeLaN parameters $(\boldsymbol{\theta},\boldsymbol{\psi})$
are frozen and the model is evaluated on all training samples
$(\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k)$,
$k=1,\dots,N_{\mathrm{train,tot}}$.
This yields joint-torque predictions
\begin{equation}
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k}
  =
  f_{\mathrm{DeLaN}}
    (\mathbf{q}_k,\dot{\mathbf{q}}_k,\ddot{\mathbf{q}}_k;
     \boldsymbol{\theta},\boldsymbol{\psi}),
\end{equation}
which we stack into a matrix
\begin{equation}
  \hat{\mathbf{T}}_{\mathrm{DeLaN,train}}
  =
  \begin{bmatrix}
    \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},1}{}^\top \\
    \vdots \\
    \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},N_{\mathrm{train,tot}}}{}^\top
  \end{bmatrix}
  \in \mathbb{R}^{N_{\mathrm{train,tot}} \times 6}.
\end{equation}

Using the corresponding motor torques
$\boldsymbol{\tau}_{\mathrm{motor},k}$, we define the joint-space residual
torques for all training samples as
\begin{equation}
  \boldsymbol{r}_{\tau,k}
  =
  \boldsymbol{\tau}_{\mathrm{motor},k}
  -
  \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k},
  \qquad k = 1,\dots,N_{\mathrm{train,tot}},
\end{equation}
and collect them in
\begin{equation}
  \mathbf{R}_{\tau,\mathrm{train}}
  =
  \begin{bmatrix}
    \boldsymbol{r}_{\tau,1}{}^\top \\
    \vdots \\
    \boldsymbol{r}_{\tau,N_{\mathrm{train,tot}}}{}^\top
  \end{bmatrix}
  \in \mathbb{R}^{N_{\mathrm{train,tot}} \times 6}.
\end{equation}

\vspace{0.75\baselineskip}

Let $H$ denote the history length (sequence length) used in Stage~2.
From the flattened arrays
$\tilde{\mathbf{Q}}_{\mathrm{train}},
 \dot{\tilde{\mathbf{Q}}}_{\mathrm{train}},
 \ddot{\tilde{\mathbf{Q}}}_{\mathrm{train}},
 \hat{\mathbf{T}}_{\mathrm{DeLaN,train}}$
we then construct overlapping sequences with a sliding window.
For each time index $k \geq H$ we define the LSTM input sequence
\begin{equation}
  \mathbf{x}_k
  =
  \Bigl[
    \mathbf{q}_{k-H+1:k},\,
    \dot{\mathbf{q}}_{k-H+1:k},\,
    \ddot{\mathbf{q}}_{k-H+1:k},\,
    \hat{\boldsymbol{\tau}}_{\mathrm{DeLaN},k-H+1:k}
  \Bigr],
\end{equation}
where, as in the main Methods section,
$\mathbf{q}_{a:b}$ denotes the stacked joint vectors
$(\mathbf{q}_a,\dots,\mathbf{q}_b)$ and analogously for
$\dot{\mathbf{q}}$, $\ddot{\mathbf{q}}$ and
$\hat{\boldsymbol{\tau}}_{\mathrm{DeLaN}}$.
The corresponding target for each sequence is chosen as the residual torque
at the final time step,
\begin{equation}
  \mathbf{y}_k = \boldsymbol{r}_{\tau,k} \in \mathbb{R}^6.
\end{equation}

Stacking all valid windows yields the Stage~2 training dataset
\begin{align}
  \mathbf{X}^{\mathrm{LSTM}}_{\mathrm{train}}
    &= \bigl\{\mathbf{x}_k\bigr\}_{k=H}^{N_{\mathrm{train,tot}}}, \\
  \mathbf{Y}^{\mathrm{LSTM}}_{\mathrm{train}}
    &= \bigl\{\mathbf{y}_k\bigr\}_{k=H}^{N_{\mathrm{train,tot}}},
\end{align}
with $N_H = N_{\mathrm{train,tot}} - H + 1$ sequences in total.
In implementation, $\mathbf{X}^{\mathrm{LSTM}}_{\mathrm{train}}$ is stored as
a tensor of shape $(N_H, H, d_{\mathrm{in}})$, where
$d_{\mathrm{in}} = 4n_{\mathrm{dof}}$ corresponds to
$(\mathbf{q},\dot{\mathbf{q}},\ddot{\mathbf{q}},\hat{\boldsymbol{\tau}}_{\mathrm{DeLaN}})$
per time step, and
$\mathbf{Y}^{\mathrm{LSTM}}_{\mathrm{train}} \in \mathbb{R}^{N_H \times 6}$.
A completely analogous construction is used for the test split.

\vspace{0.75\baselineskip}

We train an LSTM-based~\cite{GeeksforGeeks_LSTM_TensorFlow} residual model $g_{\boldsymbol{\phi}}$ on the windowed
dataset $(\mathbf{x}_k,\mathbf{y}_k)$.
As in Stage~1, the goal is to prevent single joints from dominating the
objective. To this end, we apply a train-only standardisation of both inputs
and targets, and train the network to predict \emph{scaled} residuals.

Let
$\mathbf{X}^{\mathrm{LSTM}}_{\mathrm{train}} \in \mathbb{R}^{N_H \times H \times d_{\mathrm{in}}}$
and
$\mathbf{Y}^{\mathrm{LSTM}}_{\mathrm{train}} \in \mathbb{R}^{N_H \times 6}$
denote the windowed training set.
We compute feature-wise input statistics across all windows and time steps,
\begin{equation}
  \mu_{x,j}
  =
  \frac{1}{N_H H}
  \sum_{k=1}^{N_H}\sum_{t=1}^{H} X^{\mathrm{LSTM}}_{\mathrm{train}}[k,t,j],
  \qquad
  \sigma^2_{x,j}
  =
  \frac{1}{N_H H}
  \sum_{k=1}^{N_H}\sum_{t=1}^{H}
  \bigl(X^{\mathrm{LSTM}}_{\mathrm{train}}[k,t,j] - \mu_{x,j}\bigr)^2,
\end{equation}
for $j=1,\dots,d_{\mathrm{in}}$, and collect them into vectors
$\boldsymbol{\mu}_x,\boldsymbol{\sigma}_x \in \mathbb{R}^{d_{\mathrm{in}}}$.
Analogously, we compute per-joint target statistics over windows,
\begin{equation}
  \mu_{y,i}
  =
  \frac{1}{N_H}
  \sum_{k=1}^{N_H} Y^{\mathrm{LSTM}}_{\mathrm{train}}[k,i],
  \qquad
  \sigma^2_{y,i}
  =
  \frac{1}{N_H}
  \sum_{k=1}^{N_H}
  \bigl(Y^{\mathrm{LSTM}}_{\mathrm{train}}[k,i] - \mu_{y,i}\bigr)^2,
\end{equation}
for $i=1,\dots,6$, yielding
$\boldsymbol{\mu}_y,\boldsymbol{\sigma}_y \in \mathbb{R}^{6}$.
To avoid numerical issues, standard deviations below a small threshold
$\varepsilon$ are clamped to $1$ component-wise.
Defining diagonal scaling matrices
\[
  \mathbf{W}_x = \mathrm{diag}(\boldsymbol{\sigma}_x^{-1}),
  \qquad
  \mathbf{W}_y = \mathrm{diag}(\boldsymbol{\sigma}_y^{-1}),
\]
the standardised inputs and targets are then given by
\begin{align}
  \mathbf{x}^{\mathrm{n}}_{k,t}
    &= \mathbf{W}_x \bigl(\mathbf{x}_{k,t} - \boldsymbol{\mu}_x\bigr), \\
  \boldsymbol{r}^{\mathrm{s}}_{\tau,k}
    &= \mathbf{W}_y \bigl(\boldsymbol{r}_{\tau,k} - \boldsymbol{\mu}_y\bigr),
\end{align}
with $\boldsymbol{r}_{\tau,k}=\mathbf{y}_k$.
The same $(\boldsymbol{\mu}_x,\boldsymbol{\sigma}_x,\boldsymbol{\mu}_y,\boldsymbol{\sigma}_y)$
computed on the training split are used to scale the test split.

The residual model is implemented as a two-layer LSTM with dropout regularisation
and a linear output layer,
\begin{equation}
  \hat{\boldsymbol{r}}_{\tau,k}^{\mathrm{s}}
  =
  g_{\boldsymbol{\phi}}
  \bigl(
    \mathbf{x}^{\mathrm{n}}_k
  \bigr)
  \in \mathbb{R}^{6},
\end{equation}
where $\mathbf{x}^{\mathrm{n}}_k$ denotes the standardised input window and
$\hat{\boldsymbol{r}}_{\tau,k}^{\mathrm{s}}$ the predicted \emph{scaled} residual.
Training minimises a mean-squared error on scaled residuals,
\begin{equation}
  \mathcal{L}_{\mathrm{LSTM}}(\boldsymbol{\phi})
  =
  \frac{1}{N_H}
  \sum_{k=1}^{N_H}
  \left\|
    \hat{\boldsymbol{r}}_{\tau,k}^{\mathrm{s}}
    -
    \boldsymbol{r}_{\tau,k}^{\mathrm{s}}
  \right\|_2^2,
\end{equation}
which corresponds to a balanced, per-joint normalisation via
$\boldsymbol{\sigma}_y$.
Optimisation is performed with Adam using shuffled mini-batches and an internal
validation split. The best-performing model is selected by early stopping on
the validation loss.

For reporting and for composing the final DeLaN+LSTM torque prediction, the
scaled residual is mapped back to physical units via
\begin{equation}
  \hat{\boldsymbol{r}}_{\tau,k}
  =
  \mathbf{W}_y^{-1}\hat{\boldsymbol{r}}_{\tau,k}^{\mathrm{s}}
  +
  \boldsymbol{\mu}_y,
\end{equation}
where $\mathbf{W}_y^{-1}=\mathrm{diag}(\boldsymbol{\sigma}_y)$.
